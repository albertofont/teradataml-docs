{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "c6dacea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def adjust_indentation(text, baseline_x=None):\n",
    "    \"\"\"\n",
    "    Adjusts indentation to match PDF structure where:\n",
    "    - Function name and location: 0 spaces (baseline X position)\n",
    "    - Description and content: 4 spaces (indented X position in PDF)\n",
    "    \n",
    "    Key insight from PDF X positions:\n",
    "    - __init__ line: X0=60.70 (baseline)\n",
    "    - Location line: X0=60.70 (baseline)\n",
    "    - Constructor line: X0=80.71 (indented) <- SHOULD GET 4 SPACES\n",
    "    - PARAMETERS: X0=80.71 (indented)\n",
    "    \n",
    "    Strategy: Find the first line after location that is NOT a signature continuation.\n",
    "    Signature continuations end with ) or , or match param=value pattern.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return text\n",
    "    \n",
    "    lines = text.split('\\n')\n",
    "    if not lines:\n",
    "        return text\n",
    "    \n",
    "    # Find where the signature ends (look for '=' which marks location line)\n",
    "    signature_end_index = -1\n",
    "    for i, line in enumerate(lines):\n",
    "        if '=' in line and i > 0:  # The location line has '='\n",
    "            signature_end_index = i\n",
    "            break\n",
    "    \n",
    "    if signature_end_index == -1:\n",
    "        # No location line found, return as-is\n",
    "        return text\n",
    "    \n",
    "    # Find trigger_index: first non-empty, non-signature-continuation line after location\n",
    "    trigger_index = -1\n",
    "    for i in range(signature_end_index + 1, len(lines)):\n",
    "        stripped = lines[i].strip()\n",
    "        if not stripped:\n",
    "            # Empty line, keep looking\n",
    "            continue\n",
    "        \n",
    "        # Check if it looks like a signature continuation\n",
    "        # Signature continuations: end with ) or , or match param=value\n",
    "        is_continuation = (\n",
    "            stripped.endswith(')') or \n",
    "            stripped.endswith(',') or \n",
    "            re.match(r'^[a-z_][a-z0-9_]*\\s*=', stripped, re.IGNORECASE)\n",
    "        )\n",
    "        \n",
    "        if not is_continuation:\n",
    "            # This is the first content line (description or section header)\n",
    "            trigger_index = i\n",
    "            break\n",
    "    \n",
    "    if trigger_index == -1:\n",
    "        # No content found after signature, return as-is\n",
    "        return text\n",
    "    \n",
    "    # Apply indentation: indent from trigger point onwards\n",
    "    adjusted_lines = []\n",
    "    for i, line in enumerate(lines):\n",
    "        if i < trigger_index:\n",
    "            # Before trigger: no indentation (function name and location lines)\n",
    "            adjusted_lines.append(line)\n",
    "        else:\n",
    "            # From trigger onwards: add 4 spaces if not empty\n",
    "            if line.strip():\n",
    "                adjusted_lines.append('    ' + line)\n",
    "            else:\n",
    "                adjusted_lines.append('')\n",
    "    \n",
    "    return '\\n'.join(adjusted_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "bc31e975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "REGENERATING MARKDOWN FILES WITH FIXED INDENTATION\n",
      "======================================================================\n",
      "Searching for 25 section headers...\n",
      "Found 'Teradata Package for Python Function Reference' on page 1\n",
      "Found 'teradataml: Context Manager' on page 3\n",
      "Found 'teradataml: DataFrame' on page 8\n",
      "Found 'teradataml: Time Series Functions' on page 172\n",
      "Found 'teradataml: DataFrameColumn' on page 215\n",
      "Found 'Geospatial' on page 423\n",
      "Found 'teradataml: Window Aggregates' on page 580\n",
      "Found 'teradataml: Series' on page 623\n",
      "Found 'teradataml: General Functions' on page 625\n",
      "Found 'teradataml: Plot' on page 716\n",
      "Found 'teradtaml: sdk' on page 725\n",
      "Found 'Enterprise Feature Store' on page 737\n",
      "Found 'teradataml: Bring Your Own Analytics' on page 759\n",
      "Found 'teradataml: Database 16.20.xx, 17.00.xx, 17.05.xx Analytic Functions' on page 897\n",
      "Found 'teradataml: Database 17.10.xx Analytic Functions' on page 927\n",
      "Found 'teradataml: Database 17.20.xx Analytic Functions' on page 1007\n",
      "Found 'teradataml: Database 20.00.xx Analytic Functions' on page 1220\n",
      "Found 'teradataml: Unbounded Array Framework Functions' on page 1229\n",
      "Found 'teradataml: Hyperparameter Tuning' on page 1386\n",
      "Found 'teradataml: AutoML' on page 1415\n",
      "Found 'teradataml: OpenSourceML' on page 1450\n",
      "Found 'teradataml: Vantage Analytics Library Functions' on page 1456\n",
      "Found 'teradataml: Formula' on page 1560\n",
      "Found 'teradataml: Data Preparation Functions' on page 1562\n",
      "Found 'Options' on page 1674\n",
      "Created: teradataml_function_reference/01_Introduction_and_Reference_Front_Matter.md (Pages 1-3)\n",
      "Created: teradataml_function_reference/02_teradataml_Context_Manager.md (Pages 3-8)\n",
      "Created: teradataml_function_reference/03_teradataml_DataFrame_Object_and_Methods.md (Pages 8-172)\n",
      "Created: teradataml_function_reference/04_teradataml_Time_Series_Methods.md (Pages 172-215)\n",
      "Created: teradataml_function_reference/05_teradataml_DataFrameColumn_Expressions.md (Pages 215-423)\n",
      "Created: teradataml_function_reference/06_teradataml_Geospatial_Types_and_DataFrames.md (Pages 423-580)\n",
      "Created: teradataml_function_reference/07_teradataml_Window_Aggregates.md (Pages 580-623)\n",
      "Created: teradataml_function_reference/08_teradataml_Series_Object_and_Methods.md (Pages 623-625)\n",
      "Created: teradataml_function_reference/09_teradataml_General_Functions_(Utilities_Configuration_Versioning).md (Pages 625-716)\n",
      "Created: teradataml_function_reference/10_teradataml_Plotting_Functions.md (Pages 716-725)\n",
      "Created: teradataml_function_reference/11_teradataml_SDK_Functions.md (Pages 725-737)\n",
      "Created: teradataml_function_reference/12_Enterprise_Feature_Store_Functions.md (Pages 737-759)\n",
      "Created: teradataml_function_reference/13_teradataml_Bring_Your_Own_Analytics.md (Pages 759-897)\n",
      "Created: teradataml_function_reference/14_teradataml_Database_1620xx_1700xx_1705xx_Analytic_Functions.md (Pages 897-927)\n",
      "Created: teradataml_function_reference/15_teradataml_Database_1710xx_Analytic_Functions.md (Pages 927-1007)\n",
      "Created: teradataml_function_reference/16_teradataml_Database_1720xx_Analytic_Functions.md (Pages 1007-1220)\n",
      "Created: teradataml_function_reference/17_teradataml_Database_2000xx_Analytic_Functions.md (Pages 1220-1229)\n",
      "Created: teradataml_function_reference/18_teradataml_Unbounded_Array_Framework_Functions.md (Pages 1229-1386)\n",
      "Created: teradataml_function_reference/19_teradataml_Hyperparameter_Tuning.md (Pages 1386-1415)\n",
      "Created: teradataml_function_reference/20_teradataml_AutoML.md (Pages 1415-1450)\n",
      "Created: teradataml_function_reference/21_teradataml_OpenSourceML.md (Pages 1450-1456)\n",
      "Created: teradataml_function_reference/22_teradataml_Vantage_Analytics_Library_Functions.md (Pages 1456-1560)\n",
      "Created: teradataml_function_reference/23_teradataml_Formula_Functions.md (Pages 1560-1562)\n",
      "Created: teradataml_function_reference/24_teradataml_Data_Preparation_Functions.md (Pages 1562-1674)\n",
      "Created: teradataml_function_reference/25_Options_and_Configuration.md (Pages 1674-1675)\n",
      "\n",
      "Function reference splitting complete.\n",
      "\n",
      "✓ Regeneration complete!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pymupdf as fitz\n",
    "\n",
    "# Configuration\n",
    "PDF_FILE = \"Teradata Package for Python Function Reference.pdf\"\n",
    "OUTPUT_DIR = \"teradataml_function_reference\"\n",
    "\n",
    "CHAPTER_MAP_TITLES = [\n",
    "    \"Teradata Package for Python Function Reference\",\n",
    "    \"teradataml: Context Manager\",\n",
    "    \"teradataml: DataFrame\",\n",
    "    \"teradataml: Time Series Functions\",\n",
    "    \"teradataml: DataFrameColumn\",\n",
    "    \"Geospatial\",\n",
    "    \"teradataml: Window Aggregates\",\n",
    "    \"teradataml: Series\",\n",
    "    \"teradataml: General Functions\",\n",
    "    \"teradataml: Plot\", \n",
    "    \"teradtaml: sdk\",\n",
    "    \"Enterprise Feature Store\",\n",
    "    \"teradataml: Bring Your Own Analytics\",\n",
    "    \"teradataml: Database 16.20.xx, 17.00.xx, 17.05.xx Analytic Functions\",\n",
    "    \"teradataml: Database 17.10.xx Analytic Functions\",\n",
    "    \"teradataml: Database 17.20.xx Analytic Functions\",\n",
    "    \"teradataml: Database 20.00.xx Analytic Functions\",\n",
    "    \"teradataml: Unbounded Array Framework Functions\",\n",
    "    \"teradataml: Hyperparameter Tuning\",\n",
    "    \"teradataml: AutoML\",\n",
    "    \"teradataml: OpenSourceML\",\n",
    "    \"teradataml: Vantage Analytics Library Functions\",\n",
    "    \"teradataml: Formula\",\n",
    "    \"teradataml: Data Preparation Functions\",\n",
    "    \"Options\"\n",
    "]\n",
    "\n",
    "CUSTOM_TITLE_MAP = {\n",
    "    \"Teradata Package for Python Function Reference\": \"Introduction and Reference Front Matter\",\n",
    "    \"teradataml: Context Manager\": \"teradataml Context Manager\",\n",
    "    \"teradataml: DataFrame\": \"teradataml DataFrame Object and Methods\",\n",
    "    \"teradataml: Time Series Functions\": \"teradataml Time Series Methods\",\n",
    "    \"teradataml: DataFrameColumn\": \"teradataml DataFrameColumn Expressions\",\n",
    "    \"Geospatial\": \"teradataml Geospatial Types and DataFrames\",\n",
    "    \"teradataml: Window Aggregates\": \"teradataml Window Aggregates\",\n",
    "    \"teradataml: Series\": \"teradataml Series Object and Methods\",\n",
    "    \"teradataml: General Functions\": \"teradataml General Functions (Utilities, Configuration, Versioning)\",\n",
    "    \"teradataml: Plot\": \"teradataml Plotting Functions\",\n",
    "    \"teradtaml: sdk\": \"teradataml SDK Functions\", \n",
    "    \"Enterprise Feature Store\": \"Enterprise Feature Store Functions\",\n",
    "    \"teradataml: Bring Your Own Analytics\": \"teradataml Bring Your Own Analytics\",\n",
    "    \"teradataml: Database 16.20.xx, 17.00.xx, 17.05.xx Analytic Functions\": \"teradataml Database 16.20.xx, 17.00.xx, 17.05.xx Analytic Functions\",\n",
    "    \"teradataml: Database 17.10.xx Analytic Functions\": \"teradataml Database 17.10.xx Analytic Functions\",\n",
    "    \"teradataml: Database 17.20.xx Analytic Functions\": \"teradataml Database 17.20.xx Analytic Functions\",\n",
    "    \"teradataml: Database 20.00.xx Analytic Functions\": \"teradataml Database 20.00.xx Analytic Functions\",\n",
    "    \"teradataml: Unbounded Array Framework Functions\": \"teradataml Unbounded Array Framework Functions\",\n",
    "    \"teradataml: Hyperparameter Tuning\": \"teradataml Hyperparameter Tuning\",\n",
    "    \"teradataml: AutoML\": \"teradataml AutoML\",\n",
    "    \"teradataml: OpenSourceML\": \"teradataml OpenSourceML\",\n",
    "    \"teradataml: Vantage Analytics Library Functions\": \"teradataml Vantage Analytics Library Functions\",\n",
    "    \"teradataml: Formula\": \"teradataml Formula Functions\",\n",
    "    \"teradataml: Data Preparation Functions\": \"teradataml Data Preparation Functions\",\n",
    "    \"Options\": \"Options and Configuration\"\n",
    "}\n",
    "\n",
    "JUNK_LINE_PATTERNS = [\n",
    "    re.compile(r'^9/14/2025,\\s*\\d{2}:\\d{2}$', re.IGNORECASE),          \n",
    "    re.compile(r'^9/14/2025,\\s*\\d{2}:\\d{2}\\s+Page [\\d,]+ of [\\d,]+$', re.IGNORECASE),\n",
    "    re.compile(r'^Page [\\d,]+ of [\\d,]+$', re.IGNORECASE),                             \n",
    "    re.compile(r'^PDF Export$', re.IGNORECASE),                                     \n",
    "    re.compile(r'^https?://docs\\.teradata\\.com/internal/api/webapp/print/[\\w\\d-]+$', re.IGNORECASE),                               \n",
    "]\n",
    "\n",
    "KNOWN_TITLE_PATTERNS = [\n",
    "    re.compile(re.escape(title).replace(r'\\ ', r'\\s*'), re.IGNORECASE) \n",
    "    for title in CHAPTER_MAP_TITLES\n",
    "]\n",
    "\n",
    "def clean_extracted_text(text):\n",
    "    \"\"\"Removes metadata and normalizes whitespace from PDF text.\"\"\"\n",
    "    if text is None:\n",
    "        return \"\"\n",
    "    text = text.replace('\\xa0', ' ')\n",
    "    lines = text.split('\\n')\n",
    "    cleaned_lines = []\n",
    "    all_junk_patterns = JUNK_LINE_PATTERNS + KNOWN_TITLE_PATTERNS\n",
    "\n",
    "    for line in lines:\n",
    "        line_stripped = line.strip().replace(u'\\xa0', '').replace(u'\\u200b', '')\n",
    "        if not line_stripped:\n",
    "            continue\n",
    "        is_junk = False\n",
    "        for pattern in all_junk_patterns:\n",
    "            if pattern.fullmatch(line_stripped):\n",
    "                is_junk = True\n",
    "                break\n",
    "        if not is_junk:\n",
    "            cleaned_line = line\n",
    "            for junk_pattern in JUNK_LINE_PATTERNS:\n",
    "                embedded_pattern = re.compile(junk_pattern.pattern.replace(r'^', '').replace(r'$', ''), junk_pattern.flags)\n",
    "                cleaned_line = embedded_pattern.sub('', cleaned_line)\n",
    "            cleaned_lines.append(cleaned_line)\n",
    "    result = '\\n'.join(cleaned_lines)\n",
    "    return result if result else \"\"\n",
    "\n",
    "def sanitize_title(title, index):\n",
    "    \"\"\"Maps PDF title to filename.\"\"\"\n",
    "    base_title = CUSTOM_TITLE_MAP.get(title, title)\n",
    "    prefixed_title = f\"{index:02d} {base_title}\"\n",
    "    safe_title = re.sub(r'[^\\w\\s\\(\\)\\[\\]-]', '', prefixed_title).strip()\n",
    "    safe_title = re.sub(r'\\s+', '_', safe_title)\n",
    "    return safe_title\n",
    "\n",
    "def find_chapter_page_ranges(pdf_path, titles):\n",
    "    \"\"\"Finds page ranges for each chapter in the PDF.\"\"\"\n",
    "    if not os.path.exists(pdf_path):\n",
    "        print(f\"Error: PDF file not found at '{pdf_path}'.\")\n",
    "        return []\n",
    "\n",
    "    print(f\"Searching for {len(titles)} section headers...\")\n",
    "    doc = None\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "        total_pages = doc.page_count\n",
    "        found_starts = {}\n",
    "        title_index = 0\n",
    "        \n",
    "        for page_num_0idx in range(total_pages):\n",
    "            page = doc.load_page(page_num_0idx)\n",
    "            text = page.get_text()\n",
    "            \n",
    "            if title_index < len(titles):\n",
    "                current_title = titles[title_index]\n",
    "                escaped_title = re.escape(current_title)\n",
    "                flexible_pattern_str = escaped_title.replace(r'\\ ', r'[\\s\\n]+')\n",
    "                search_pattern = re.compile(flexible_pattern_str, re.IGNORECASE | re.DOTALL)\n",
    "                match = search_pattern.search(text)\n",
    "                \n",
    "                if match:\n",
    "                    start_page_1idx = page_num_0idx + 1\n",
    "                    start_offset = match.start()\n",
    "                    found_starts[current_title] = (start_page_1idx, start_offset)\n",
    "                    print(f\"Found '{current_title}' on page {start_page_1idx}\")\n",
    "                    title_index += 1\n",
    "        \n",
    "        final_map = []\n",
    "        sorted_starts = sorted(found_starts.items(), key=lambda item: item[1][0])\n",
    "        \n",
    "        for i, (title, (start_page, start_offset)) in enumerate(sorted_starts):\n",
    "            if i < len(sorted_starts) - 1:\n",
    "                next_start_page, next_start_offset = sorted_starts[i+1][1]\n",
    "                end_page = next_start_page\n",
    "            else:\n",
    "                end_page = total_pages\n",
    "            final_map.append((title, start_page, end_page, start_offset))\n",
    "        \n",
    "        return final_map\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during PDF processing: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return []\n",
    "    finally:\n",
    "        if doc:\n",
    "            doc.close()\n",
    "\n",
    "def create_and_split_files(pdf_path, chapter_map):\n",
    "    \"\"\"Splits PDF into markdown files using the updated adjust_indentation().\"\"\"\n",
    "    if not chapter_map:\n",
    "        print(\"Cannot create files: Chapter map is empty.\")\n",
    "        return\n",
    "\n",
    "    if not os.path.exists(OUTPUT_DIR):\n",
    "        os.makedirs(OUTPUT_DIR)\n",
    "        print(f\"Created output directory: {OUTPUT_DIR}\")\n",
    "\n",
    "    doc = None\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "\n",
    "        for idx, (title, start_page_1idx, end_page_1idx, start_offset) in enumerate(chapter_map):\n",
    "            try:\n",
    "                start_page_0idx = start_page_1idx - 1\n",
    "                end_page_0idx = end_page_1idx - 1\n",
    "                safe_filename = sanitize_title(title, idx + 1)\n",
    "                output_filename = os.path.join(OUTPUT_DIR, f\"{safe_filename}.md\")\n",
    "                \n",
    "                chapter_text = []\n",
    "                \n",
    "                start_page = doc.load_page(start_page_0idx)\n",
    "                text = start_page.get_text()\n",
    "                chapter_text.append(text[start_offset:])\n",
    "\n",
    "                for page_num in range(start_page_0idx + 1, end_page_0idx):\n",
    "                    page = doc.load_page(page_num)\n",
    "                    text = page.get_text()\n",
    "                    chapter_text.append(text)\n",
    "\n",
    "                if start_page_0idx < end_page_0idx:\n",
    "                    end_page_content = doc.load_page(end_page_0idx).get_text()\n",
    "                    end_offset = len(end_page_content)\n",
    "                    if idx + 1 < len(chapter_map):\n",
    "                        next_title, next_start_page, _, next_start_offset = chapter_map[idx+1]\n",
    "                        if next_start_page == end_page_1idx:\n",
    "                            end_offset = next_start_offset\n",
    "                    chapter_text.append(end_page_content[:end_offset])\n",
    "\n",
    "                raw_text = \"\".join(chapter_text)\n",
    "                cleaned_content = clean_extracted_text(raw_text)\n",
    "                if cleaned_content is None:\n",
    "                    print(f\"WARNING: clean_extracted_text returned None for {title}\")\n",
    "                    cleaned_content = \"\"\n",
    "                adjusted_content = adjust_indentation(cleaned_content)\n",
    "                if adjusted_content is None:\n",
    "                    print(f\"WARNING: adjust_indentation returned None for {title}\")\n",
    "                    adjusted_content = cleaned_content\n",
    "\n",
    "                with open(output_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "                    f.write(f\"# {CUSTOM_TITLE_MAP.get(title, title)}\\n\\n\") \n",
    "                    f.write(adjusted_content)\n",
    "                \n",
    "                print(f\"Created: {output_filename} (Pages {start_page_1idx}-{end_page_1idx})\")\n",
    "            except Exception as e:\n",
    "                print(f\"ERROR processing chapter '{title}': {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "        \n",
    "        print(\"\\nFunction reference splitting complete.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during file creation: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    finally:\n",
    "        if doc:\n",
    "            doc.close()\n",
    "\n",
    "# Run the pipeline\n",
    "print(\"=\" * 70)\n",
    "print(\"REGENERATING MARKDOWN FILES WITH FIXED INDENTATION\")\n",
    "print(\"=\" * 70)\n",
    "calculated_map = find_chapter_page_ranges(PDF_FILE, CHAPTER_MAP_TITLES)\n",
    "\n",
    "if calculated_map:\n",
    "    create_and_split_files(PDF_FILE, calculated_map)\n",
    "    print(\"\\n✓ Regeneration complete!\")\n",
    "else:\n",
    "    print(\"ERROR: Could not generate chapter map.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "faeb0efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table created: teradataml_function_table.md with 1179 entries\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "directory = \"teradataml_function_reference\"\n",
    "functions = []\n",
    "\n",
    "def is_valid_function_candidate(line):\n",
    "    # Only allow valid Python identifiers (no spaces, no punctuation, no non-ASCII)\n",
    "    return re.match(r'^[A-Za-z_][A-Za-z0-9_]*$', line) is not None\n",
    "\n",
    "def extract_from_file(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    i = 0\n",
    "    results = []\n",
    "    while i < len(lines):\n",
    "        line = lines[i].rstrip('\\n')\n",
    "        stripped = line.strip()\n",
    "        \n",
    "        # FIX PART B: Proper indentation validation (updated for new 4-space indentation)\n",
    "        # Count all leading whitespace (regular spaces, tabs, and non-breaking spaces)\n",
    "        leading_ws = len(line) - len(line.lstrip(' \\t\\xa0'))\n",
    "        # With new indentation: function names are at 4 spaces, skip deeply indented lines (8+)\n",
    "        # that are likely code examples or parameter details\n",
    "        if leading_ws >= 8 and stripped and is_valid_function_candidate(stripped):\n",
    "            # Line is too deeply indented to be a function definition\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        # Exception: If line is \"Functions\" and next line is a function signature, use previous line as name\n",
    "        if stripped == \"Functions\" and i > 0 and i+1 < len(lines):\n",
    "            next_line = lines[i+1].strip()\n",
    "            # Check if next line is a function signature (e.g. Antiselect(data=None, ...)\n",
    "            if re.match(r'^[A-Za-z_][A-Za-z0-9_]*\\s*\\(', next_line):\n",
    "                prev_line = lines[i-1].rstrip('\\n')\n",
    "                prev_stripped = prev_line.strip()\n",
    "                # Check previous line is a valid candidate\n",
    "                if prev_stripped and is_valid_function_candidate(prev_stripped):\n",
    "                    name = prev_stripped\n",
    "                    location = name\n",
    "                    k_start = i+2\n",
    "                    k = k_start\n",
    "                    while k < len(lines) and not lines[k].strip():\n",
    "                        k += 1\n",
    "                    while k < len(lines) and lines[k].strip() and not lines[k].strip().upper().startswith('DESCRIPTION') and lines[k].strip() not in (\n",
    "                        'PARAMETERS:', 'RAISES:', 'RETURNS:', 'EXAMPLES:', 'NOTES:'):\n",
    "                        k += 1\n",
    "                    desc_lines = []\n",
    "                    if k < len(lines) and lines[k].strip().upper().startswith('DESCRIPTION'):\n",
    "                        desc_line = lines[k]\n",
    "                        if ':' in desc_line and desc_line.strip().endswith(':'):\n",
    "                            k += 1\n",
    "                        else:\n",
    "                            possible_parts = desc_line.split(':', 1)\n",
    "                            possible = possible_parts[1].strip() if len(possible_parts) > 1 else ''\n",
    "                            if possible:\n",
    "                                desc_lines = [possible]\n",
    "                                k += 1\n",
    "                            else:\n",
    "                                k += 1\n",
    "                    else:\n",
    "                        while k < len(lines) and lines[k].strip() not in ('PARAMETERS:', 'RAISES:', 'RETURNS:', 'EXAMPLES:', 'NOTES:'):\n",
    "                            line = lines[k].strip()\n",
    "                            if line:\n",
    "                                desc_lines.append(line)\n",
    "                            k += 1\n",
    "                    while k < len(lines):\n",
    "                        l = lines[k]\n",
    "                        if l.strip() == '':\n",
    "                            k += 1\n",
    "                            continue\n",
    "                        stripped_l = l.strip()\n",
    "                        if stripped_l in ('PARAMETERS:', 'RAISES:', 'RETURNS:', 'EXAMPLES:', 'NOTES:'):\n",
    "                            break\n",
    "                        # Check indentation level to determine if it's part of description\n",
    "                        leading_ws_l = len(l) - len(l.lstrip(' \\t\\xa0'))\n",
    "                        if leading_ws_l >= 4:\n",
    "                            desc_lines.append(l.strip())\n",
    "                            k += 1\n",
    "                        else:\n",
    "                            break\n",
    "                    desc_text = ' '.join(desc_lines).strip()\n",
    "                    first_sent = ''\n",
    "                    if desc_text:\n",
    "                        sentences = re.split(r'(?<=\\.)\\s+', desc_text)\n",
    "                        first_sent = sentences[0].strip()\n",
    "                        word_count = len(first_sent.split())\n",
    "                        if word_count < 10 and len(sentences) > 1:\n",
    "                            first_sent = (first_sent + ' ' + sentences[1].strip()).strip()\n",
    "                    results.append({\n",
    "                        'name': name,\n",
    "                        'location': location,\n",
    "                        'description': first_sent\n",
    "                    })\n",
    "                    i = k\n",
    "                    continue\n",
    "                else:\n",
    "                    i += 1\n",
    "                    continue\n",
    "        # Normal candidate function name\n",
    "        elif stripped and is_valid_function_candidate(stripped):\n",
    "            name = stripped\n",
    "        else:\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        # Find the next non-empty line\n",
    "        j = i + 1\n",
    "        while j < len(lines) and not lines[j].strip():\n",
    "            j += 1\n",
    "\n",
    "        location = None\n",
    "        k_start = None\n",
    "        function_indent_level = leading_ws  # Remember indentation level of function name\n",
    "\n",
    "        if j < len(lines):\n",
    "            next_line = lines[j].strip()\n",
    "            if '=' in next_line:\n",
    "                location_line = next_line\n",
    "                if ' = ' in location_line:\n",
    "                    location = location_line.split(' = ')[0].strip()\n",
    "                else:\n",
    "                    location = location_line.split('=')[0].strip()\n",
    "                k_start = j + 1\n",
    "            elif next_line.startswith(name + '('):\n",
    "                location = name\n",
    "                k_start = j + 1\n",
    "            else:\n",
    "                # New: Accept a dotted module/function path as a location line\n",
    "                if re.match(r'^[A-Za-z_][A-Za-z0-9_]*(?:\\.[A-Za-z_][A-Za-z0-9_]*)+$', next_line):\n",
    "                    # Example: teradataml.common.formula.Formula.all_columns\n",
    "                    location = next_line\n",
    "                    k_start = j + 1\n",
    "                else:\n",
    "                    i += 1\n",
    "                    continue\n",
    "\n",
    "        if location is None:\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        # NEW: Simplified description extraction\n",
    "        # Description lines come after the location line and BEFORE the first section keyword\n",
    "        # (DESCRIPTION:, PARAMETERS:, RAISES:, RETURNS:, EXAMPLES:, NOTES:)\n",
    "        # Description can either:\n",
    "        # 1. Start with \"DESCRIPTION:\" header (followed by content on next lines or same line after colon)\n",
    "        # 2. Start immediately as plain text (no header)\n",
    "        \n",
    "        # FIX: Handle BOTH adjusted (4-space) and unadjusted (0-space) files\n",
    "        # If function was at 0 spaces, description is also at 0 spaces\n",
    "        # If function was at 4 spaces, description is at 4+ spaces\n",
    "        \n",
    "        k = k_start\n",
    "        desc_lines = []\n",
    "        \n",
    "        # Skip empty lines after location\n",
    "        while k < len(lines) and not lines[k].strip():\n",
    "            k += 1\n",
    "        \n",
    "        # Check if first content line is \"DESCRIPTION:\"\n",
    "        if k < len(lines) and lines[k].strip().upper().startswith('DESCRIPTION'):\n",
    "            desc_line = lines[k]\n",
    "            if ':' in desc_line and desc_line.strip().endswith(':'):\n",
    "                # DESCRIPTION: on its own line, content starts on next line\n",
    "                k += 1\n",
    "            else:\n",
    "                # DESCRIPTION: with content on same line\n",
    "                possible_parts = desc_line.split(':', 1)\n",
    "                possible = possible_parts[1].strip() if len(possible_parts) > 1 else ''\n",
    "                if possible:\n",
    "                    desc_lines = [possible]\n",
    "                k += 1\n",
    "        \n",
    "        # Now collect description lines until we hit a section keyword\n",
    "        # For 0-space functions: collect lines at 0 spaces (same level as function)\n",
    "        # For 4-space functions: collect lines at 4+ spaces\n",
    "        min_desc_indent = function_indent_level\n",
    "        \n",
    "        while k < len(lines):\n",
    "            l = lines[k]\n",
    "            if l.strip() == '':\n",
    "                k += 1\n",
    "                continue\n",
    "            \n",
    "            stripped_l = l.strip()\n",
    "            # Check for section keywords\n",
    "            if stripped_l in ('PARAMETERS:', 'RAISES:', 'RETURNS:', 'EXAMPLES:', 'NOTES:'):\n",
    "                break\n",
    "            \n",
    "            # Check indentation: description lines should be at the same indent level as function or greater\n",
    "            leading_ws_l = len(l) - len(l.lstrip(' \\t\\xa0'))\n",
    "            \n",
    "            if function_indent_level == 0:\n",
    "                # For 0-space functions, accept description at 0 or 4+ spaces\n",
    "                # 0 spaces = content at same level\n",
    "                # 4+ spaces = indented content (parameters, etc.)\n",
    "                # Stop at section keywords regardless\n",
    "                if leading_ws_l in (0, 4, 8, 12):  # Normal description indentation levels\n",
    "                    # But if it's a section keyword, stop\n",
    "                    if stripped_l not in ('PARAMETERS:', 'RAISES:', 'RETURNS:', 'EXAMPLES:', 'NOTES:'):\n",
    "                        desc_lines.append(l.strip())\n",
    "                        k += 1\n",
    "                    else:\n",
    "                        break\n",
    "                else:\n",
    "                    k += 1\n",
    "            else:\n",
    "                # For 4-space functions, accept description at 4+ spaces only\n",
    "                if leading_ws_l >= function_indent_level:\n",
    "                    desc_lines.append(l.strip())\n",
    "                    k += 1\n",
    "                else:\n",
    "                    # Less indent: end of current function\n",
    "                    break\n",
    "        \n",
    "        desc_text = ' '.join(desc_lines).strip()\n",
    "        first_sent = ''\n",
    "        if desc_text:\n",
    "            sentences = re.split(r'(?<=\\.)\\s+', desc_text)\n",
    "            first_sent = sentences[0].strip()\n",
    "            word_count = len(first_sent.split())\n",
    "            if word_count < 10 and len(sentences) > 1:\n",
    "                first_sent = (first_sent + ' ' + sentences[1].strip()).strip()\n",
    "        \n",
    "        results.append({\n",
    "            'name': name,\n",
    "            'location': location,\n",
    "            'description': first_sent\n",
    "        })\n",
    "        i = k\n",
    "    return results\n",
    "\n",
    "if not os.path.isdir(directory):\n",
    "    print(f\"Directory not found: {directory}\")\n",
    "else:\n",
    "    for idx in range(2, 26):\n",
    "        fname = None\n",
    "        for f in os.listdir(directory):\n",
    "            if f.startswith(f\"{idx:02d}_\") and f.endswith('.md'):\n",
    "                fname = f\n",
    "                break\n",
    "        if not fname:\n",
    "            continue\n",
    "        path = os.path.join(directory, fname)\n",
    "        try:\n",
    "            extracted = extract_from_file(path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing {path}: {e}\")\n",
    "            continue\n",
    "        for item in extracted:\n",
    "            item['file'] = fname\n",
    "            functions.append(item)\n",
    "table_lines = [\n",
    "    \"| File | Name | Location | Description |\",\n",
    "    \"|------|------|----------|-------------|\",\n",
    "]\n",
    "for func in functions:\n",
    "    desc = (func.get('description') or '').replace('|', '\\\\|')\n",
    "    table_lines.append(f\"| {func.get('file','')} | {func.get('name','')} | {func.get('location','')} | {desc} |\")\n",
    "output_path = 'teradataml_function_table.md'\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(\"# TeradataML Function Reference Table\\n\\n\")\n",
    "    f.write('\\n'.join(table_lines) + '\\n')\n",
    "print(f\"Table created: {output_path} with {len(functions)} entries\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de71bd6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f26ac00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "2ab14481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed duplicates. Total unique rows: 1168\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicate rows from teradataml_function_table.md\n",
    "file_path = 'teradataml_function_table.md'\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# Find the header lines (first 3 lines: title, header, separator)\n",
    "header_lines = lines[:3]\n",
    "\n",
    "# Parse the data rows\n",
    "data_rows = []\n",
    "seen = set()\n",
    "\n",
    "for line in lines[3:]:\n",
    "    stripped = line.strip()\n",
    "    if stripped:\n",
    "        # Split by '|' and strip each part\n",
    "        parts = [p.strip() for p in stripped.split('|')[1:-1]]  # Remove empty first and last\n",
    "        row_tuple = tuple(parts)\n",
    "        if row_tuple not in seen:\n",
    "            seen.add(row_tuple)\n",
    "            data_rows.append(line)\n",
    "\n",
    "# Write back the file\n",
    "with open(file_path, 'w', encoding='utf-8') as f:\n",
    "    f.writelines(header_lines)\n",
    "    f.writelines(data_rows)\n",
    "\n",
    "print(f\"Removed duplicates. Total unique rows: {len(data_rows)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08303f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated teradataml_function_table.md with Synonyms/Keywords column based on File, Name, Location match.\n",
      "Number of rows changed: 1167\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# File paths\n",
    "file1_path = 'teradataml_function_table.md'\n",
    "file2_path = 'teradataml_function_table copy_synonym3.md'\n",
    "\n",
    "# Read the files\n",
    "with open(file1_path, 'r', encoding='utf-8') as f:\n",
    "    lines1 = f.readlines()\n",
    "\n",
    "with open(file2_path, 'r', encoding='utf-8') as f:\n",
    "    lines2 = f.readlines()\n",
    "\n",
    "# Build synonyms dict from file2 data rows (data starts at index 3)\n",
    "synonyms_dict = {}\n",
    "for line in lines2[3:]:\n",
    "    if line.strip():\n",
    "        parts = [p.strip() for p in line.strip().split('|')[1:-1]]\n",
    "        if len(parts) >= 5:\n",
    "            key = (parts[0], parts[1], parts[2])\n",
    "            synonyms_dict[key] = parts[4]\n",
    "\n",
    "# Rebuild the first file content with robust header/separator handling\n",
    "new_lines = []\n",
    "# Title\n",
    "new_lines.append(lines1[0] if lines1 else '# TeradataML Function Reference Table\\n')\n",
    "# Parse header into parts\n",
    "if len(lines1) > 1 and '|' in lines1[1]:\n",
    "    header_parts = [p.strip() for p in lines1[1].strip().split('|')[1:-1]]\n",
    "else:\n",
    "    header_parts = ['File', 'Name', 'Location', 'Description']\n",
    "# Ensure header has 5 columns\n",
    "target_cols = ['File', 'Name', 'Location', 'Description', 'Synonyms/Keywords']\n",
    "for i in range(len(target_cols)):\n",
    "    if i >= len(header_parts) or not header_parts[i]:\n",
    "        header_parts = header_parts[:i] + [target_cols[i]] + header_parts[i:]\n",
    "header_parts = header_parts[:5]\n",
    "new_lines.append('| ' + ' | '.join(header_parts) + ' |\\n')\n",
    "# Separator\n",
    "sep_parts = ['-' * max(6, len(h)) for h in header_parts]\n",
    "new_lines.append('| ' + ' | '.join(sep_parts) + ' |\\n')\n",
    "\n",
    "num_changed = 0\n",
    "for i in range(3, len(lines1)):\n",
    "    line = lines1[i]\n",
    "    if line.strip() and not line.strip().startswith('|------'):\n",
    "        parts = [p.strip() for p in line.strip().split('|')[1:-1]]\n",
    "        # normalize to at least 4 cols\n",
    "        while len(parts) < 4:\n",
    "            parts.append('')\n",
    "        key = (parts[0], parts[1], parts[2])\n",
    "        syn = synonyms_dict.get(key, '')\n",
    "        if syn:\n",
    "            num_changed += 1\n",
    "        parts = parts[:4] + [syn]\n",
    "        new_lines.append('| ' + ' | '.join(parts) + ' |\\n')\n",
    "\n",
    "# Write back to file1\n",
    "with open(file1_path, 'w', encoding='utf-8') as f:\n",
    "    f.writelines(new_lines)\n",
    "\n",
    "print(f\"Updated {file1_path} with Synonyms/Keywords column based on File, Name, Location match.\")\n",
    "print(f\"Number of rows changed: {num_changed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4220130",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013900e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311gcopilot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
