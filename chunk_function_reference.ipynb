{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c6dacea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for 25 section headers in 'Teradata Package for Python Function Reference.pdf'...\n",
      "Found 'Teradata Package for Python Function Reference' on page 1 at offset 858 using resilient search.\n",
      "Found 'teradataml: Context Manager' on page 3 at offset 1113 using resilient search.\n",
      "Found 'teradataml: DataFrame' on page 8 at offset 1721 using resilient search.\n",
      "Found 'Teradata Package for Python Function Reference' on page 1 at offset 858 using resilient search.\n",
      "Found 'teradataml: Context Manager' on page 3 at offset 1113 using resilient search.\n",
      "Found 'teradataml: DataFrame' on page 8 at offset 1721 using resilient search.\n",
      "Found 'teradataml: Time Series Functions' on page 172 at offset 4284 using resilient search.\n",
      "Found 'teradataml: Time Series Functions' on page 172 at offset 4284 using resilient search.\n",
      "Found 'teradataml: DataFrameColumn' on page 215 at offset 2266 using resilient search.\n",
      "Found 'teradataml: DataFrameColumn' on page 215 at offset 2266 using resilient search.\n",
      "Found 'Geospatial' on page 423 at offset 1988 using resilient search.\n",
      "Found 'Geospatial' on page 423 at offset 1988 using resilient search.\n",
      "Found 'teradataml: Window Aggregates' on page 580 at offset 2706 using resilient search.\n",
      "Found 'teradataml: Window Aggregates' on page 580 at offset 2706 using resilient search.\n",
      "Found 'teradataml: Series' on page 623 at offset 173 using resilient search.\n",
      "Found 'teradataml: General Functions' on page 625 at offset 356 using resilient search.\n",
      "Found 'teradataml: Series' on page 623 at offset 173 using resilient search.\n",
      "Found 'teradataml: General Functions' on page 625 at offset 356 using resilient search.\n",
      "Found 'teradataml: Plot' on page 716 at offset 1073 using resilient search.\n",
      "Found 'teradtaml: sdk' on page 725 at offset 909 using resilient search.\n",
      "Found 'Enterprise Feature Store' on page 737 at offset 532 using resilient search.\n",
      "Found 'teradataml: Plot' on page 716 at offset 1073 using resilient search.\n",
      "Found 'teradtaml: sdk' on page 725 at offset 909 using resilient search.\n",
      "Found 'Enterprise Feature Store' on page 737 at offset 532 using resilient search.\n",
      "Found 'teradataml: Bring Your Own Analytics' on page 759 at offset 552 using resilient search.\n",
      "Found 'teradataml: Bring Your Own Analytics' on page 759 at offset 552 using resilient search.\n",
      "Found 'teradataml: Database 16.20.xx, 17.00.xx, 17.05.xx Analytic Functions' on page 897 at offset 2803 using resilient search.\n",
      "Found 'teradataml: Database 17.10.xx Analytic Functions' on page 927 at offset 574 using resilient search.\n",
      "Found 'teradataml: Database 16.20.xx, 17.00.xx, 17.05.xx Analytic Functions' on page 897 at offset 2803 using resilient search.\n",
      "Found 'teradataml: Database 17.10.xx Analytic Functions' on page 927 at offset 574 using resilient search.\n",
      "Found 'teradataml: Database 17.20.xx Analytic Functions' on page 1007 at offset 311 using resilient search.\n",
      "Found 'teradataml: Database 17.20.xx Analytic Functions' on page 1007 at offset 311 using resilient search.\n",
      "Found 'teradataml: Database 20.00.xx Analytic Functions' on page 1220 at offset 2930 using resilient search.\n",
      "Found 'teradataml: Unbounded Array Framework Functions' on page 1229 at offset 2566 using resilient search.\n",
      "Found 'teradataml: Database 20.00.xx Analytic Functions' on page 1220 at offset 2930 using resilient search.\n",
      "Found 'teradataml: Unbounded Array Framework Functions' on page 1229 at offset 2566 using resilient search.\n",
      "Found 'teradataml: Hyperparameter Tuning' on page 1386 at offset 1391 using resilient search.\n",
      "Found 'teradataml: AutoML' on page 1415 at offset 2220 using resilient search.\n",
      "Found 'teradataml: Hyperparameter Tuning' on page 1386 at offset 1391 using resilient search.\n",
      "Found 'teradataml: AutoML' on page 1415 at offset 2220 using resilient search.\n",
      "Found 'teradataml: OpenSourceML' on page 1450 at offset 479 using resilient search.\n",
      "Found 'teradataml: Vantage Analytics Library Functions' on page 1456 at offset 1839 using resilient search.\n",
      "Found 'teradataml: OpenSourceML' on page 1450 at offset 479 using resilient search.\n",
      "Found 'teradataml: Vantage Analytics Library Functions' on page 1456 at offset 1839 using resilient search.\n",
      "Found 'teradataml: Formula' on page 1560 at offset 831 using resilient search.\n",
      "Found 'teradataml: Data Preparation Functions' on page 1562 at offset 233 using resilient search.\n",
      "Found 'teradataml: Formula' on page 1560 at offset 831 using resilient search.\n",
      "Found 'teradataml: Data Preparation Functions' on page 1562 at offset 233 using resilient search.\n",
      "Found 'Options' on page 1674 at offset 1181 using resilient search.\n",
      "\n",
      "--- Calculated Chapter Map (Title, Start Page, End Page, Start Offset) ---\n",
      "(\"Teradata Package for Python Function Reference\", 1, 3, 858),\n",
      "(\"teradataml: Context Manager\", 3, 8, 1113),\n",
      "(\"teradataml: DataFrame\", 8, 172, 1721),\n",
      "(\"teradataml: Time Series Functions\", 172, 215, 4284),\n",
      "(\"teradataml: DataFrameColumn\", 215, 423, 2266),\n",
      "(\"Geospatial\", 423, 580, 1988),\n",
      "(\"teradataml: Window Aggregates\", 580, 623, 2706),\n",
      "(\"teradataml: Series\", 623, 625, 173),\n",
      "(\"teradataml: General Functions\", 625, 716, 356),\n",
      "(\"teradataml: Plot\", 716, 725, 1073),\n",
      "(\"teradtaml: sdk\", 725, 737, 909),\n",
      "(\"Enterprise Feature Store\", 737, 759, 532),\n",
      "(\"teradataml: Bring Your Own Analytics\", 759, 897, 552),\n",
      "(\"teradataml: Database 16.20.xx, 17.00.xx, 17.05.xx Analytic Functions\", 897, 927, 2803),\n",
      "(\"teradataml: Database 17.10.xx Analytic Functions\", 927, 1007, 574),\n",
      "(\"teradataml: Database 17.20.xx Analytic Functions\", 1007, 1220, 311),\n",
      "(\"teradataml: Database 20.00.xx Analytic Functions\", 1220, 1229, 2930),\n",
      "(\"teradataml: Unbounded Array Framework Functions\", 1229, 1386, 2566),\n",
      "(\"teradataml: Hyperparameter Tuning\", 1386, 1415, 1391),\n",
      "(\"teradataml: AutoML\", 1415, 1450, 2220),\n",
      "(\"teradataml: OpenSourceML\", 1450, 1456, 479),\n",
      "(\"teradataml: Vantage Analytics Library Functions\", 1456, 1560, 1839),\n",
      "(\"teradataml: Formula\", 1560, 1562, 831),\n",
      "(\"teradataml: Data Preparation Functions\", 1562, 1674, 233),\n",
      "(\"Options\", 1674, 1675, 1181),\n",
      "----------------------------------------------------------\n",
      "\n",
      "Found 'Options' on page 1674 at offset 1181 using resilient search.\n",
      "\n",
      "--- Calculated Chapter Map (Title, Start Page, End Page, Start Offset) ---\n",
      "(\"Teradata Package for Python Function Reference\", 1, 3, 858),\n",
      "(\"teradataml: Context Manager\", 3, 8, 1113),\n",
      "(\"teradataml: DataFrame\", 8, 172, 1721),\n",
      "(\"teradataml: Time Series Functions\", 172, 215, 4284),\n",
      "(\"teradataml: DataFrameColumn\", 215, 423, 2266),\n",
      "(\"Geospatial\", 423, 580, 1988),\n",
      "(\"teradataml: Window Aggregates\", 580, 623, 2706),\n",
      "(\"teradataml: Series\", 623, 625, 173),\n",
      "(\"teradataml: General Functions\", 625, 716, 356),\n",
      "(\"teradataml: Plot\", 716, 725, 1073),\n",
      "(\"teradtaml: sdk\", 725, 737, 909),\n",
      "(\"Enterprise Feature Store\", 737, 759, 532),\n",
      "(\"teradataml: Bring Your Own Analytics\", 759, 897, 552),\n",
      "(\"teradataml: Database 16.20.xx, 17.00.xx, 17.05.xx Analytic Functions\", 897, 927, 2803),\n",
      "(\"teradataml: Database 17.10.xx Analytic Functions\", 927, 1007, 574),\n",
      "(\"teradataml: Database 17.20.xx Analytic Functions\", 1007, 1220, 311),\n",
      "(\"teradataml: Database 20.00.xx Analytic Functions\", 1220, 1229, 2930),\n",
      "(\"teradataml: Unbounded Array Framework Functions\", 1229, 1386, 2566),\n",
      "(\"teradataml: Hyperparameter Tuning\", 1386, 1415, 1391),\n",
      "(\"teradataml: AutoML\", 1415, 1450, 2220),\n",
      "(\"teradataml: OpenSourceML\", 1450, 1456, 479),\n",
      "(\"teradataml: Vantage Analytics Library Functions\", 1456, 1560, 1839),\n",
      "(\"teradataml: Formula\", 1560, 1562, 831),\n",
      "(\"teradataml: Data Preparation Functions\", 1562, 1674, 233),\n",
      "(\"Options\", 1674, 1675, 1181),\n",
      "----------------------------------------------------------\n",
      "\n",
      "Created: teradataml_function_reference/01_Introduction_and_Reference_Front_Matter.md (Pages 1-3)\n",
      "Created: teradataml_function_reference/02_teradataml_Context_Manager.md (Pages 3-8)\n",
      "Created: teradataml_function_reference/01_Introduction_and_Reference_Front_Matter.md (Pages 1-3)\n",
      "Created: teradataml_function_reference/02_teradataml_Context_Manager.md (Pages 3-8)\n",
      "Created: teradataml_function_reference/03_teradataml_DataFrame_Object_and_Methods.md (Pages 8-172)\n",
      "Created: teradataml_function_reference/03_teradataml_DataFrame_Object_and_Methods.md (Pages 8-172)\n",
      "Created: teradataml_function_reference/04_teradataml_Time_Series_Methods.md (Pages 172-215)\n",
      "Created: teradataml_function_reference/04_teradataml_Time_Series_Methods.md (Pages 172-215)\n",
      "Created: teradataml_function_reference/05_teradataml_DataFrameColumn_Expressions.md (Pages 215-423)\n",
      "Created: teradataml_function_reference/05_teradataml_DataFrameColumn_Expressions.md (Pages 215-423)\n",
      "Created: teradataml_function_reference/06_teradataml_Geospatial_Types_and_DataFrames.md (Pages 423-580)\n",
      "Created: teradataml_function_reference/06_teradataml_Geospatial_Types_and_DataFrames.md (Pages 423-580)\n",
      "Created: teradataml_function_reference/07_teradataml_Window_Aggregates.md (Pages 580-623)\n",
      "Created: teradataml_function_reference/08_teradataml_Series_Object_and_Methods.md (Pages 623-625)\n",
      "Created: teradataml_function_reference/07_teradataml_Window_Aggregates.md (Pages 580-623)\n",
      "Created: teradataml_function_reference/08_teradataml_Series_Object_and_Methods.md (Pages 623-625)\n",
      "Created: teradataml_function_reference/09_teradataml_General_Functions_(Utilities_Configuration_Versioning).md (Pages 625-716)\n",
      "Created: teradataml_function_reference/10_teradataml_Plotting_Functions.md (Pages 716-725)\n",
      "Created: teradataml_function_reference/11_teradataml_SDK_Functions.md (Pages 725-737)\n",
      "Created: teradataml_function_reference/09_teradataml_General_Functions_(Utilities_Configuration_Versioning).md (Pages 625-716)\n",
      "Created: teradataml_function_reference/10_teradataml_Plotting_Functions.md (Pages 716-725)\n",
      "Created: teradataml_function_reference/11_teradataml_SDK_Functions.md (Pages 725-737)\n",
      "Created: teradataml_function_reference/12_Enterprise_Feature_Store_Functions.md (Pages 737-759)\n",
      "Created: teradataml_function_reference/12_Enterprise_Feature_Store_Functions.md (Pages 737-759)\n",
      "Created: teradataml_function_reference/13_teradataml_Bring_Your_Own_Analytics.md (Pages 759-897)\n",
      "Created: teradataml_function_reference/13_teradataml_Bring_Your_Own_Analytics.md (Pages 759-897)\n",
      "Created: teradataml_function_reference/14_teradataml_Database_1620xx_1700xx_1705xx_Analytic_Functions.md (Pages 897-927)\n",
      "Created: teradataml_function_reference/14_teradataml_Database_1620xx_1700xx_1705xx_Analytic_Functions.md (Pages 897-927)\n",
      "Created: teradataml_function_reference/15_teradataml_Database_1710xx_Analytic_Functions.md (Pages 927-1007)\n",
      "Created: teradataml_function_reference/15_teradataml_Database_1710xx_Analytic_Functions.md (Pages 927-1007)\n",
      "Created: teradataml_function_reference/16_teradataml_Database_1720xx_Analytic_Functions.md (Pages 1007-1220)\n",
      "Created: teradataml_function_reference/17_teradataml_Database_2000xx_Analytic_Functions.md (Pages 1220-1229)\n",
      "Created: teradataml_function_reference/16_teradataml_Database_1720xx_Analytic_Functions.md (Pages 1007-1220)\n",
      "Created: teradataml_function_reference/17_teradataml_Database_2000xx_Analytic_Functions.md (Pages 1220-1229)\n",
      "Created: teradataml_function_reference/18_teradataml_Unbounded_Array_Framework_Functions.md (Pages 1229-1386)\n",
      "Created: teradataml_function_reference/18_teradataml_Unbounded_Array_Framework_Functions.md (Pages 1229-1386)\n",
      "Created: teradataml_function_reference/19_teradataml_Hyperparameter_Tuning.md (Pages 1386-1415)\n",
      "Created: teradataml_function_reference/19_teradataml_Hyperparameter_Tuning.md (Pages 1386-1415)\n",
      "Created: teradataml_function_reference/20_teradataml_AutoML.md (Pages 1415-1450)\n",
      "Created: teradataml_function_reference/21_teradataml_OpenSourceML.md (Pages 1450-1456)\n",
      "Created: teradataml_function_reference/20_teradataml_AutoML.md (Pages 1415-1450)\n",
      "Created: teradataml_function_reference/21_teradataml_OpenSourceML.md (Pages 1450-1456)\n",
      "Created: teradataml_function_reference/22_teradataml_Vantage_Analytics_Library_Functions.md (Pages 1456-1560)\n",
      "Created: teradataml_function_reference/23_teradataml_Formula_Functions.md (Pages 1560-1562)\n",
      "Created: teradataml_function_reference/22_teradataml_Vantage_Analytics_Library_Functions.md (Pages 1456-1560)\n",
      "Created: teradataml_function_reference/23_teradataml_Formula_Functions.md (Pages 1560-1562)\n",
      "Created: teradataml_function_reference/24_teradataml_Data_Preparation_Functions.md (Pages 1562-1674)\n",
      "Created: teradataml_function_reference/25_Options_and_Configuration.md (Pages 1674-1675)\n",
      "\n",
      "Function reference splitting complete.\n",
      "Created: teradataml_function_reference/24_teradataml_Data_Preparation_Functions.md (Pages 1562-1674)\n",
      "Created: teradataml_function_reference/25_Options_and_Configuration.md (Pages 1674-1675)\n",
      "\n",
      "Function reference splitting complete.\n"
     ]
    }
   ],
   "source": [
    "import pymupdf as fitz\n",
    "import os\n",
    "import re\n",
    "\n",
    "# --- Configuration ---\n",
    "# NOTE: This should point to the newly uploaded Function Reference PDF.\n",
    "PDF_FILE = \"Teradata Package for Python Function Reference.pdf\"\n",
    "OUTPUT_DIR = \"teradataml_function_reference\"\n",
    "\n",
    "# List of all 25 expected section headers in sequential order, verified against the PDF.\n",
    "CHAPTER_MAP_TITLES = [\n",
    "    \"Teradata Package for Python Function Reference\",\n",
    "    \"teradataml: Context Manager\",\n",
    "    \"teradataml: DataFrame\",\n",
    "    \"teradataml: Time Series Functions\",\n",
    "    \"teradataml: DataFrameColumn\",\n",
    "    \"Geospatial\",\n",
    "    \"teradataml: Window Aggregates\",\n",
    "    \"teradataml: Series\",\n",
    "    \"teradataml: General Functions\",\n",
    "    \n",
    "    \"teradataml: Plot\", \n",
    "    \"teradtaml: sdk\", # Reinstated user's typo, assuming this is the string in the PDF\n",
    "    \"Enterprise Feature Store\",\n",
    "    \"teradataml: Bring Your Own Analytics\",\n",
    "    \"teradataml: Database 16.20.xx, 17.00.xx, 17.05.xx Analytic Functions\",\n",
    "    \"teradataml: Database 17.10.xx Analytic Functions\",\n",
    "    \"teradataml: Database 17.20.xx Analytic Functions\",\n",
    "    \"teradataml: Database 20.00.xx Analytic Functions\",\n",
    "    \"teradataml: Unbounded Array Framework Functions\",\n",
    "    \"teradataml: Hyperparameter Tuning\",\n",
    "    \"teradataml: AutoML\",\n",
    "    \"teradataml: OpenSourceML\",\n",
    "    \"teradataml: Vantage Analytics Library Functions\",\n",
    "    \"teradataml: Formula\",\n",
    "    \"teradataml: Data Preparation Functions\",\n",
    "    \"Options\"\n",
    "]\n",
    "\n",
    "# Mapping from ORIGINAL_TITLE (used for searching) to the SEARCH-OPTIMIZED TITLE (used for file naming).\n",
    "# This map is now fully complete and verified.\n",
    "CUSTOM_TITLE_MAP = {\n",
    "    \"Teradata Package for Python Function Reference\": \"Introduction and Reference Front Matter\",\n",
    "    \"teradataml: Context Manager\": \"teradataml Context Manager\",\n",
    "    \"teradataml: DataFrame\": \"teradataml DataFrame Object and Methods\",\n",
    "    \"teradataml: Time Series Functions\": \"teradataml Time Series Methods\",\n",
    "    \"teradataml: DataFrameColumn\": \"teradataml DataFrameColumn Expressions\",\n",
    "    \"Geospatial\": \"teradataml Geospatial Types and DataFrames\",\n",
    "    \"teradataml: Window Aggregates\": \"teradataml Window Aggregates\",\n",
    "    \"teradataml: Series\": \"teradataml Series Object and Methods\",\n",
    "    \"teradataml: General Functions\": \"teradataml General Functions (Utilities, Configuration, Versioning)\",\n",
    "    \n",
    "    \"teradataml: Plot\": \"teradataml Plotting Functions\",\n",
    "    \"teradtaml: sdk\": \"teradataml SDK Functions\", \n",
    "    \"Enterprise Feature Store\": \"Enterprise Feature Store Functions\",\n",
    "    \"teradataml: Bring Your Own Analytics\": \"teradataml Bring Your Own Analytics\",\n",
    "    \"teradataml: Database 16.20.xx, 17.00.xx, 17.05.xx Analytic Functions\": \"teradataml Database 16.20.xx, 17.00.xx, 17.05.xx Analytic Functions\",\n",
    "    \"teradataml: Database 17.10.xx Analytic Functions\": \"teradataml Database 17.10.xx Analytic Functions\",\n",
    "    \"teradataml: Database 17.20.xx Analytic Functions\": \"teradataml Database 17.20.xx Analytic Functions\",\n",
    "    \"teradataml: Database 20.00.xx Analytic Functions\": \"teradataml Database 20.00.xx Analytic Functions\",\n",
    "    \"teradataml: Unbounded Array Framework Functions\": \"teradataml Unbounded Array Framework Functions\",\n",
    "    \"teradataml: Hyperparameter Tuning\": \"teradataml Hyperparameter Tuning\",\n",
    "    \"teradataml: AutoML\": \"teradataml AutoML\",\n",
    "    \"teradataml: OpenSourceML\": \"teradataml OpenSourceML\",\n",
    "    \"teradataml: Vantage Analytics Library Functions\": \"teradataml Vantage Analytics Library Functions\",\n",
    "    \"teradataml: Formula\": \"teradataml Formula Functions\",\n",
    "    \"teradataml: Data Preparation Functions\": \"teradataml Data Preparation Functions\",\n",
    "    \"Options\": \"Options and Configuration\"\n",
    "}\n",
    "\n",
    "\n",
    "# List of regex patterns to match known junk/metadata lines, compiled for efficiency.\n",
    "# The `^...$` anchors ensure we only remove lines that *only* contain the junk.\n",
    "JUNK_LINE_PATTERNS = [\n",
    "    # 1. Date/Time: Match the specific junk date/time format\n",
    "    re.compile(r'^9/14/2025,\\s*\\d{2}:\\d{2}$', re.IGNORECASE),          \n",
    "    # 2. Combined Date/Time and Page: Match lines like \"9/14/2025, 16:50 Page 1,498 of 1,675\"\n",
    "    re.compile(r'^9/14/2025,\\s*\\d{2}:\\d{2}\\s+Page [\\d,]+ of [\\d,]+$', re.IGNORECASE),\n",
    "    # 3. Page numbers: Matches 'Page 1,457 of 1,675' (with commas in numbers)\n",
    "    re.compile(r'^Page [\\d,]+ of [\\d,]+$', re.IGNORECASE),                             \n",
    "    # 4. PDF Export: Literal match (as agreed)\n",
    "    re.compile(r'^PDF Export$', re.IGNORECASE),                                     \n",
    "    # 5. Specific Teradata Documentation Print URL (Safely targets the known junk URL structure)\n",
    "    re.compile(r'^https?://docs\\.teradata\\.com/internal/api/webapp/print/[\\w\\d-]+$', re.IGNORECASE),                               \n",
    "]\n",
    "\n",
    "# Pre-compile known titles into regex patterns to remove the repeated header line \n",
    "# that often follows the H1 we insert. We allow for flexible whitespace.\n",
    "KNOWN_TITLE_PATTERNS = [\n",
    "    re.compile(re.escape(title).replace(r'\\ ', r'\\s*'), re.IGNORECASE) \n",
    "    for title in CHAPTER_MAP_TITLES\n",
    "]\n",
    "\n",
    "def clean_extracted_text(text):\n",
    "    \"\"\"\n",
    "    Removes common header/footer metadata and repeated chapter titles from \n",
    "    the raw extracted PDF text based on predefined patterns.\n",
    "    \"\"\"\n",
    "    lines = text.split('\\n')\n",
    "    cleaned_lines = []\n",
    "    \n",
    "    # Combine all patterns for a single check\n",
    "    all_junk_patterns = JUNK_LINE_PATTERNS + KNOWN_TITLE_PATTERNS\n",
    "\n",
    "    for line in lines:\n",
    "        # Strip leading/trailing whitespace and the common non-breaking space/zero width space\n",
    "        line_stripped = line.strip().replace(u'\\xa0', '').replace(u'\\u200b', '')\n",
    "        \n",
    "        # Skip truly empty lines\n",
    "        if not line_stripped:\n",
    "            continue\n",
    "\n",
    "        is_junk = False\n",
    "        \n",
    "        # Check against all known junk patterns\n",
    "        for pattern in all_junk_patterns:\n",
    "            # Check for a full match against the stripped line\n",
    "            if pattern.fullmatch(line_stripped):\n",
    "                is_junk = True\n",
    "                break\n",
    "        \n",
    "        if not is_junk:\n",
    "            # Remove embedded junk patterns from within the line\n",
    "            cleaned_line = line\n",
    "            for junk_pattern in JUNK_LINE_PATTERNS:\n",
    "                # Create a version without anchors for embedded removal\n",
    "                embedded_pattern = re.compile(junk_pattern.pattern.replace(r'^', '').replace(r'$', ''), junk_pattern.flags)\n",
    "                cleaned_line = embedded_pattern.sub('', cleaned_line)\n",
    "            cleaned_lines.append(cleaned_line)\n",
    "        \n",
    "    # Rejoin the lines\n",
    "    return '\\n'.join(cleaned_lines)\n",
    "\n",
    "\n",
    "def sanitize_title(title, index):\n",
    "    \"\"\"\n",
    "    Maps the original PDF title to the search-optimized title,\n",
    "    adds the chapter index, and cleans it for safe filesystem usage.\n",
    "    \"\"\"\n",
    "    # 1. Apply the search-optimized name\n",
    "    base_title = CUSTOM_TITLE_MAP.get(title, title)\n",
    "    \n",
    "    # 2. Add the prefix\n",
    "    prefixed_title = f\"{index:02d} {base_title}\"\n",
    "\n",
    "    # 3. Sanitize for filesystem (replace illegal characters with underscore)\n",
    "    safe_title = re.sub(r'[^\\w\\s\\(\\)\\[\\]-]', '', prefixed_title).strip()\n",
    "    safe_title = re.sub(r'\\s+', '_', safe_title)\n",
    "    \n",
    "    return safe_title\n",
    "\n",
    "def find_chapter_page_ranges(pdf_path, titles):\n",
    "    \"\"\"\n",
    "    Scans the PDF for the provided titles and returns a list of \n",
    "    (title, start_page_1idx, end_page_1idx, start_offset) tuples, \n",
    "    where start_offset is the character index where the title is found on the start page.\n",
    "    Uses a resilient regex to handle potential PDF text extraction issues (like internal newlines).\n",
    "    \"\"\"\n",
    "    if not os.path.exists(pdf_path):\n",
    "        print(f\"Error: PDF file not found at '{pdf_path}'.\")\n",
    "        return []\n",
    "\n",
    "    print(f\"Searching for {len(titles)} section headers in '{pdf_path}'...\")\n",
    "    \n",
    "    doc = None\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "        total_pages = doc.page_count\n",
    "        \n",
    "        # Stores {title: (start_page_1idx, start_offset)}\n",
    "        found_starts = {}\n",
    "        title_index = 0\n",
    "        \n",
    "        # Iterate through all pages to find the start of each section\n",
    "        for page_num_0idx in range(total_pages):\n",
    "            page = doc.load_page(page_num_0idx)\n",
    "            text = page.get_text()\n",
    "            \n",
    "            # Search for the next expected title\n",
    "            if title_index < len(titles):\n",
    "                current_title = titles[title_index]\n",
    "                \n",
    "                # --- SINGLE-STAGE RESILIENT REGEX ---\n",
    "                # 1. Escape the full title to treat all punctuation literally (like periods).\n",
    "                escaped_title = re.escape(current_title)\n",
    "                \n",
    "                # 2. Replace all escaped spaces with a pattern that matches one or more \n",
    "                #    whitespace or newline characters, making it resilient to PDF text breaks.\n",
    "                flexible_pattern_str = escaped_title.replace(r'\\ ', r'[\\s\\n]+')\n",
    "                    \n",
    "                search_pattern = re.compile(flexible_pattern_str, re.IGNORECASE | re.DOTALL)\n",
    "                \n",
    "                # We use the raw text for searching\n",
    "                match = search_pattern.search(text)\n",
    "                \n",
    "                if match:\n",
    "                    start_page_1idx = page_num_0idx + 1\n",
    "                    start_offset = match.start()\n",
    "                    found_starts[current_title] = (start_page_1idx, start_offset)\n",
    "                    print(f\"Found '{current_title}' on page {start_page_1idx} at offset {start_offset} using resilient search.\")\n",
    "                    title_index += 1\n",
    "        \n",
    "        # Now determine the end pages and build the final map\n",
    "        # Store as (title, start_page_1idx, end_page_1idx, start_offset)\n",
    "        final_map = []\n",
    "        \n",
    "        # Create a list of (title, start_page, start_offset) tuples sorted by page number\n",
    "        sorted_starts = sorted(found_starts.items(), key=lambda item: item[1][0])\n",
    "        \n",
    "        # Iterate through the found sections to calculate the end page\n",
    "        for i, (title, (start_page, start_offset)) in enumerate(sorted_starts):\n",
    "            if i < len(sorted_starts) - 1:\n",
    "                # End page is the page where the next section starts\n",
    "                next_start_page, next_start_offset = sorted_starts[i+1][1]\n",
    "                end_page = next_start_page\n",
    "            else:\n",
    "                # Last section ends on the last page of the document\n",
    "                end_page = total_pages\n",
    "            \n",
    "            final_map.append((title, start_page, end_page, start_offset))\n",
    "        \n",
    "        return final_map\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during PDF processing: {e}\")\n",
    "        return []\n",
    "    finally:\n",
    "        if doc:\n",
    "            doc.close()\n",
    "\n",
    "def create_and_split_files(pdf_path, chapter_map):\n",
    "    \"\"\"\n",
    "    Splits the PDF content into Markdown files based on the determined chapter map,\n",
    "    using the start offset for precise content extraction on the first page.\n",
    "    \"\"\"\n",
    "    if not chapter_map:\n",
    "        print(\"Cannot create files: Chapter map is empty.\")\n",
    "        return\n",
    "\n",
    "    if not os.path.exists(OUTPUT_DIR):\n",
    "        os.makedirs(OUTPUT_DIR)\n",
    "        print(f\"Created output directory: {OUTPUT_DIR}\")\n",
    "\n",
    "    doc = None\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "\n",
    "        for idx, (title, start_page_1idx, end_page_1idx, start_offset) in enumerate(chapter_map):\n",
    "            \n",
    "            start_page_0idx = start_page_1idx - 1\n",
    "            end_page_0idx = end_page_1idx - 1\n",
    "            \n",
    "            # Use the new sanitize_title which handles prefixing and mapping\n",
    "            safe_filename = sanitize_title(title, idx + 1)\n",
    "            output_filename = os.path.join(\n",
    "                OUTPUT_DIR, \n",
    "                f\"{safe_filename}.md\"\n",
    "            )\n",
    "            \n",
    "            chapter_text = []\n",
    "            \n",
    "            # 1. Process the START page (start_page_0idx)\n",
    "            start_page = doc.load_page(start_page_0idx)\n",
    "            text = start_page.get_text()\n",
    "            \n",
    "            # Extract content starting from the title's offset\n",
    "            chapter_text.append(text[start_offset:])\n",
    "\n",
    "            # 2. Process all MIDDLE pages (if any)\n",
    "            for page_num in range(start_page_0idx + 1, end_page_0idx):\n",
    "                page = doc.load_page(page_num)\n",
    "                text = page.get_text()\n",
    "                chapter_text.append(text)\n",
    "\n",
    "            # 3. Process the END page (if it's different from the start page)\n",
    "            if start_page_0idx < end_page_0idx:\n",
    "                end_page_content = doc.load_page(end_page_0idx).get_text()\n",
    "                \n",
    "                # Find where the next chapter starts on this page, if it's the same page\n",
    "                end_offset = len(end_page_content) # Default to full page\n",
    "                if idx + 1 < len(chapter_map):\n",
    "                    next_title, next_start_page, _, next_start_offset = chapter_map[idx+1]\n",
    "                    if next_start_page == end_page_1idx:\n",
    "                        end_offset = next_start_offset\n",
    "                \n",
    "                chapter_text.append(end_page_content[:end_offset])\n",
    "\n",
    "            # 4. Join and clean the text\n",
    "            raw_text = \"\".join(chapter_text)\n",
    "            cleaned_content = clean_extracted_text(raw_text)\n",
    "\n",
    "            # 5. Write the text to the Markdown file\n",
    "            with open(output_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "                # Add the chapter title as a top-level header for easy reading\n",
    "                f.write(f\"# {CUSTOM_TITLE_MAP.get(title, title)}\\n\\n\") \n",
    "                f.write(cleaned_content)\n",
    "            \n",
    "            print(f\"Created: {output_filename} (Pages {start_page_1idx}-{end_page_1idx})\")\n",
    "        \n",
    "        print(\"\\nFunction reference splitting complete.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during file creation: {e}\")\n",
    "    finally:\n",
    "        if doc:\n",
    "            doc.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # 1. Programmatically determine the page ranges\n",
    "    calculated_map = find_chapter_page_ranges(PDF_FILE, CHAPTER_MAP_TITLES)\n",
    "    \n",
    "    if calculated_map:\n",
    "        print(\"\\n--- Calculated Chapter Map (Title, Start Page, End Page, Start Offset) ---\")\n",
    "        for title, start, end, offset in calculated_map:\n",
    "            print(f\"(\\\"{title}\\\", {start}, {end}, {offset}),\")\n",
    "        print(\"----------------------------------------------------------\\n\")\n",
    "        \n",
    "        # 2. Create the files based on the calculated map\n",
    "        create_and_split_files(PDF_FILE, calculated_map)\n",
    "    else:\n",
    "        print(\"Could not generate a chapter map. Please ensure the PDF file is correctly named and contains the expected titles.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faeb0efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "# Directory containing the generated markdown chapters\n",
    "directory = \"teradataml_function_reference\"\n",
    "\n",
    "# Collect parsed function entries\n",
    "functions = []\n",
    "\n",
    "# Helper: extract function entries from a single markdown file\n",
    "def extract_from_file(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    i = 0\n",
    "    results = []\n",
    "    while i < len(lines):\n",
    "        line = lines[i].rstrip('\\n')\n",
    "        stripped = line.strip()\n",
    "\n",
    "        # Candidate function name: not a header, not a chapter title and not a location line\n",
    "        if stripped and not line[0].isspace() and not stripped.startswith('#') and not stripped.lower().startswith('teradataml:') and '=' not in stripped and stripped not in (\n",
    "            'PARAMETERS:', 'RAISES:', 'RETURNS:', 'EXAMPLES:', 'NOTES:', 'DESCRIPTION:') and not any(char in stripped for char in [' ', ':']) and not stripped.startswith(('>>>', 'from ', 'import ', 'Example', 'The ')) and not any(keyword in stripped for keyword in ['df', '[', 'print', 'load_example_data', '>>>']):\n",
    "            name = stripped\n",
    "\n",
    "            # Find the next non-empty line (likely the location with an '=' sign)\n",
    "            j = i + 1\n",
    "            while j < len(lines) and not lines[j].strip():\n",
    "                j += 1\n",
    "\n",
    "            if j < len(lines) and '=' in lines[j]:\n",
    "                location_line = lines[j].strip()\n",
    "                # left side of '=' is the full object path\n",
    "                if ' = ' in location_line:\n",
    "                    location = location_line.split(' = ')[0].strip()\n",
    "                else:\n",
    "                    location = location_line.split('=')[0].strip()\n",
    "\n",
    "                # Move to the line after the location and skip blank lines\n",
    "                k = j + 1\n",
    "                # Skip continuation lines of the location (lines that start with lowercase, assuming descriptions start with uppercase)\n",
    "                while k < len(lines) and lines[k].strip() and not lines[k].strip()[0].isupper() and not lines[k].strip().upper().startswith('DESCRIPTION') and lines[k].strip() not in (\n",
    "                    'PARAMETERS:', 'RAISES:', 'RETURNS:', 'EXAMPLES:', 'NOTES:'):\n",
    "                    k += 1\n",
    "                # Skip any remaining blank lines\n",
    "                while k < len(lines) and not lines[k].strip():\n",
    "                    k += 1\n",
    "\n",
    "                desc_lines = []  # Initialize desc_lines\n",
    "\n",
    "                # Handle optional \"DESCRIPTION:\" marker\n",
    "                if k < len(lines) and lines[k].strip().upper().startswith('DESCRIPTION'):\n",
    "                    # If the line is exactly 'DESCRIPTION:' advance; if it contains text after colon, keep it\n",
    "                    desc_line = lines[k]\n",
    "                    if ':' in desc_line and desc_line.strip().endswith(':'):\n",
    "                        k += 1\n",
    "                    else:\n",
    "                        # DESCRIPTION: <text on same line>\n",
    "                        possible_parts = desc_line.split(':', 1)\n",
    "                        possible = possible_parts[1].strip() if len(possible_parts) > 1 else ''\n",
    "                        if possible:\n",
    "                            desc_lines = [possible]\n",
    "                            k += 1\n",
    "                        else:\n",
    "                            k += 1\n",
    "                else:\n",
    "                    # No DESCRIPTION: marker, collect description lines until section header\n",
    "                    while k < len(lines) and lines[k].strip() not in ('PARAMETERS:', 'RAISES:', 'RETURNS:', 'EXAMPLES:', 'NOTES:'):\n",
    "                        line = lines[k].strip()\n",
    "                        if line:\n",
    "                            desc_lines.append(line)\n",
    "                        k += 1\n",
    "\n",
    "                # Collect any additional indented lines (for cases with DESCRIPTION: followed by indented notes)\n",
    "                while k < len(lines):\n",
    "                    l = lines[k]\n",
    "                    if l.strip() == '':\n",
    "                        k += 1\n",
    "                        continue\n",
    "                    # Consider as indented if starts with space, tab or NBSP\n",
    "                    if l.startswith(' ') or l.startswith('\\t') or l.startswith('\\u00A0'):\n",
    "                        # strip leading/trailing whitespace\n",
    "                        desc_lines.append(l.strip())\n",
    "                        k += 1\n",
    "                    else:\n",
    "                        break\n",
    "\n",
    "                desc_text = ' '.join(desc_lines).strip()\n",
    "\n",
    "                # Pick first sentence (or two if the first is very short)\n",
    "                first_sent = ''\n",
    "                if desc_text:\n",
    "                    sentences = re.split(r'(?<=\\.)\\s+', desc_text)\n",
    "                    first_sent = sentences[0].strip()\n",
    "                    word_count = len(first_sent.split())\n",
    "                    if word_count < 10 and len(sentences) > 1:\n",
    "                        first_sent = (first_sent + ' ' + sentences[1].strip()).strip()\n",
    "\n",
    "                results.append({\n",
    "                    'name': name,\n",
    "                    'location': location,\n",
    "                    'description': first_sent\n",
    "                })\n",
    "\n",
    "                # Advance i to continue parsing after the description block we consumed\n",
    "                i = k\n",
    "            else:\n",
    "                # No location found, continue scanning\n",
    "                i += 1\n",
    "        else:\n",
    "            i += 1\n",
    "\n",
    "    return results\n",
    "\n",
    "# Iterate over the expected files 02..25\n",
    "if not os.path.isdir(directory):\n",
    "    print(f\"Directory not found: {directory}\")\n",
    "else:\n",
    "    for idx in range(2, 26):\n",
    "        fname = None\n",
    "        for f in os.listdir(directory):\n",
    "            if f.startswith(f\"{idx:02d}_\") and f.endswith('.md'):\n",
    "                fname = f\n",
    "                break\n",
    "        if not fname:\n",
    "            continue\n",
    "\n",
    "        path = os.path.join(directory, fname)\n",
    "        try:\n",
    "            extracted = extract_from_file(path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing {path}: {e}\")\n",
    "            continue\n",
    "\n",
    "        for item in extracted:\n",
    "            item['file'] = fname\n",
    "            functions.append(item)\n",
    "\n",
    "# Build a Markdown table and write the output file\n",
    "table_lines = [\n",
    "    \"| File | Name | Location | Description |\",\n",
    "    \"|------|------|----------|-------------|\",\n",
    "]\n",
    "for func in functions:\n",
    "    desc = (func.get('description') or '').replace('|', '\\\\|')\n",
    "    table_lines.append(f\"| {func.get('file','')} | {func.get('name','')} | {func.get('location','')} | {desc} |\")\n",
    "\n",
    "output_path = 'teradataml_function_table.md'\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(\"# TeradataML Function Reference Table\\n\\n\")\n",
    "    f.write('\\n'.join(table_lines) + '\\n')\n",
    "\n",
    "print(f\"Table created: {output_path} with {len(functions)} entries\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08303f32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6668bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bf3d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table created: teradataml_function_table.md\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "directory = \"teradataml_function_reference\"\n",
    "functions = []\n",
    "\n",
    "for i in range(2, 26):\n",
    "    filename = None\n",
    "    for f in os.listdir(directory):\n",
    "        if f.startswith(f\"{i:02d}_\") and f.endswith(\".md\"):\n",
    "            filename = f\n",
    "            break\n",
    "    if not filename:\n",
    "        continue\n",
    "    \n",
    "    filepath = os.path.join(directory, filename)\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "    \n",
    "    i_line = 0\n",
    "    while i_line < len(lines):\n",
    "        line = lines[i_line].strip()\n",
    "        if line and not line.startswith('#') and not line.startswith('teradataml:') and '=' not in line and not line in ['PARAMETERS:', 'RAISES:', 'RETURNS:', 'EXAMPLES:', 'NOTES:']:\n",
    "            # possible function name\n",
    "            name = line\n",
    "            if i_line + 1 < len(lines) and '=' in lines[i_line + 1]:\n",
    "                location_line = lines[i_line + 1].strip()\n",
    "                if ' = ' in location_line:\n",
    "                    location = location_line.split(' = ')[0]\n",
    "                    # collect description\n",
    "                    i_line += 2  # after location\n",
    "                    if i_line < len(lines) and lines[i_line].strip() == \"DESCRIPTION:\":\n",
    "                        i_line += 1  # skip DESCRIPTION:\n",
    "                    desc_lines = []\n",
    "                    while i_line < len(lines):\n",
    "                        line = lines[i_line].strip()\n",
    "                        if line and line in ['PARAMETERS:', 'RAISES:', 'RETURNS:', 'EXAMPLES:', 'NOTES:']:\n",
    "                            break\n",
    "                        if line:\n",
    "                            desc_lines.append(line)\n",
    "                        i_line += 1\n",
    "                    desc_text = ' '.join(desc_lines).strip()\n",
    "                    if desc_text:\n",
    "                        # split sentences\n",
    "                        sentences = re.split(r'(?<=\\.)\\s+', desc_text)\n",
    "                        first_sent = sentences[0].strip() if sentences else ''\n",
    "                        word_count = len(first_sent.split())\n",
    "                        if word_count < 10 and len(sentences) > 1:\n",
    "                            first_sent += ' ' + sentences[1].strip()\n",
    "                        functions.append({'name': name, 'location': location, 'description': first_sent})\n",
    "                    else:\n",
    "                        pass  # no desc\n",
    "                else:\n",
    "                    i_line += 1\n",
    "            else:\n",
    "                i_line += 1\n",
    "        else:\n",
    "            i_line += 1\n",
    "\n",
    "# create the table\n",
    "table = \"| Name | Location | Description |\\n|------|----------|-------------|\\n\"\n",
    "for func in functions:\n",
    "    # escape | in description\n",
    "    desc = func['description'].replace('|', '\\\\|')\n",
    "    table += f\"| {func['name']} | {func['location']} | {desc} |\\n\"\n",
    "\n",
    "# write to file\n",
    "with open(\"teradataml_function_table.md\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"# TeradataML Function Reference Table\\n\\n\")\n",
    "    f.write(table)\n",
    "\n",
    "print(\"Table created: teradataml_function_table.md\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311gcopilot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
