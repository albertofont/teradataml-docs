{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "130fdab4",
   "metadata": {},
   "source": [
    "## Import Dependencies\n",
    "\n",
    "Imports the required libraries for PDF parsing and file operations:\n",
    "- `pymupdf` (fitz): For reading and extracting text from PDF files\n",
    "- `os`: For file and directory operations\n",
    "- `re`: For regex pattern matching in text processing\n",
    "- `json`: For working with JSON data (e.g., exporting raw metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "018183a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymupdf as fitz\n",
    "import os\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047b1d75",
   "metadata": {},
   "source": [
    "## Configuration: Chapter Map & PDF Metadata\n",
    "\n",
    "Defines the structure of the PDF document:\n",
    "- **CHAPTER_MAP**: A list of tuples containing (chapter_title, start_page, end_page) for each chapter/section in the Teradata Package for Python User Guide\n",
    "- **PDF_FILE**: The path to the PDF file to be parsed\n",
    "\n",
    "The chapter map is used to split the extracted content into separate markdown files per chapter, preserving document structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "bf3c38bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration: Chapter Map & PDF Metadata\n",
    "CHAPTER_MAP = [\n",
    "    (\"Table of Contents\", 3, 6),\n",
    "    (\"Introduction to Teradata Package for Python\", 7, 26),\n",
    "    (\"Installing, Uninstalling, and Upgrading Teradata Package for Python\", 27, 31),\n",
    "    (\"teradataml Components\", 32, 42),\n",
    "    (\"DataFrames Setup and Basics (Sources, Non-Default DB, UAF)\", 43, 60),\n",
    "    (\"DataFrame Manipulation (Core API)\", 61, 164),\n",
    "    (\"DataFrame Metadata, Rotation, Saving, and Export\", 165, 202),\n",
    "    (\"Executing Python Functions Inside Database Engine 20\", 203, 242),\n",
    "    (\"teradataml DataFrame Column\", 243, 279),\n",
    "    (\"teradataml Window Aggregates\", 280, 288),\n",
    "    (\"Context to Teradata Vantage\", 289, 301),\n",
    "    (\"teradataml Options\", 302, 319),\n",
    "    (\"teradataml Utility and General Functions\", 320, 399),\n",
    "    (\"teradataml Open-Source Machine Learning Functions\", 400, 457),\n",
    "    (\"Script Methods (SCRIPT Table Operator)\", 458, 477),\n",
    "    (\"Series (DataFrame Column Sequence)\", 478, 481),\n",
    "    (\"BYOM (Bring Your Own Model) Management\", 482, 519),\n",
    "    (\"Working with Geospatial Data\", 520, 571),\n",
    "    (\"Exploratory Data Analysis (EDA UI)\", 572, 576),\n",
    "    (\"Plotting in teradataml\", 577, 611),\n",
    "    (\"Hyperparameter Tuning in teradataml\", 612, 693),\n",
    "    (\"AutoML Overview and Methods\", 694, 718),\n",
    "    (\"AutoML Examples\", 719, 1108),\n",
    "    (\"AutoDataPrep\", 1109, 1150),\n",
    "    (\"Feature Store in teradataml\", 1151, 1190),\n",
    "    (\"Using Teradata Vantage Analytic Functions with Teradata Package for Python\", 1191, 1235),\n",
    "    (\"Appendix A: Teradata Package for Python Limitations and Considerations\", 1236, 1260),\n",
    "    (\"Appendix B: Using teradataml with Native Object Store\", 1261, 1276),\n",
    "    (\"Appendix C: teradataml Extension with SQLAlchemy\", 1277, 1295),\n",
    "    (\"Appendix D: Data Type Mapping\", 1296, 1297),\n",
    "    (\"Appendix E: Additional Information\", 1298, 1301)\n",
    "]\n",
    "\n",
    "PDF_FILE = \"Teradata Package for Python User Guide.pdf\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63df0a2",
   "metadata": {},
   "source": [
    "## Constants: Font Sizes & Constants\n",
    "\n",
    "Defines all constants for PDF parsing:\n",
    "- **Font sizes**: H2 (17.95), H3 (15.96), H4 (11.55) for heading detection\n",
    "- **BODY_TEXT_SIZE**: 10.5 points (standard text)\n",
    "- **CODE_FONT**: 'Consolas' (identifies code blocks)\n",
    "- **BOLD_FLAG**: 16 (bit flag for bold text)\n",
    "- **Bullet character sets**: BLACK_BULLETS (•), WHITE_BULLETS (◦) for nested lists\n",
    "- **Y_MERGE_TOLERANCE**: 5 points (merge lines within this vertical distance)\n",
    "- **APPENDIX_START_INDEX**: 26 (index where appendices begin in chapter map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "130fdab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X-coordinate indentation mapping (generalized across PDF)\n",
    "# Rule: Indentation strictly based on X thresholds (0/2/4/6 spaces)\n",
    "# Rule: Nested levels must have at least +2 more spaces than parent\n",
    "X_INDENT_RANGES = [\n",
    "    (50, 75, 0, None, False),        # Level 0: X ~50-75, 0 spaces, no bullet (main content)\n",
    "    (59, 73, 0, '*', True),          # Level 0 black bullet: X ~59-73, 0 spaces, with bullet •\n",
    "    (72, 85, 2, '*', True),          # Level 1 bullet: X ~72-85, 2 spaces, with bullet • or ◦\n",
    "    (77, 90, 2, None, False),        # Level 1 content: X ~77-90, 2 spaces, no bullet\n",
    "    (85, 115, 4, '*', True),         # Level 2 bullet/content: X ~85-115, 4 spaces, with bullet or no bullet\n",
    "    (111, 130, 4, None, False),      # Level 2 content: X ~111-130, 4 spaces, no bullet\n",
    "    (125, 150, 6, None, False),      # Level 3 content: X ~125-150, 6 spaces, no bullet\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8244e3",
   "metadata": {},
   "source": [
    "## Define PDF Constants\n",
    "\n",
    "Defines all font sizes, styles, and tolerance thresholds used throughout PDF parsing:\n",
    "- **Font sizes**: H2 (17.95pt), H3 (15.96pt), H4 (11.55-13.96pt) for heading detection\n",
    "- **Body text**: 10.5pt standard text size\n",
    "- **Code font**: Consolas (identifies code blocks)\n",
    "- **Bold flag**: 16 (bit flag for bold text)\n",
    "- **Bullet characters**: BLACK_BULLETS (•, •, *) and WHITE_BULLETS (◦) for nested lists\n",
    "- **Tolerances**: Y_MERGE (5pt), Font tolerance (0.01pt)\n",
    "- **APPENDIX_START_INDEX**: Chapter 26 is where appendices begin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "b75ec509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants: PDF Font Sizes, Styles, and Tolerance Thresholds\n",
    "TOLERANCE = 0.01\n",
    "H2_SIZE_SPECIFIC = 17.954999923706055\n",
    "H3_SIZE_GENERIC = 15.960000038146973\n",
    "H4_SIZE_ARIAL_BLACK = 11.550000190734863\n",
    "H4_SIZE_BOLD_SECONDARY = 13.96500015258789\n",
    "BODY_TEXT_SIZE = 10.5\n",
    "BOLD_FLAG = 16\n",
    "CODE_FONT = 'Consolas'\n",
    "H4_FONT = 'Arial-Black'\n",
    "Y_MERGE_TOLERANCE = 5.0  # Lines within 5 points vertically are part of same row\n",
    "APPENDIX_START_INDEX = 26  # Chapter index where appendices begin\n",
    "\n",
    "# Bullet character detection (consolidated - used in multiple places)\n",
    "BULLET_CHARS = {'•', '\\u2022', '◦', '\\u25E6', '*', '-'}\n",
    "WHITE_BULLETS = {'◦', '\\u25E6'}  # Nested bullets\n",
    "BLACK_BULLETS = {'•', '\\u2022', '*'}  # Main level bullets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "2c07832c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging Setup\n",
    "import logging\n",
    "\n",
    "# Configure logging to write to a file\n",
    "logging.basicConfig(\n",
    "    filename='extraction_log.txt',\n",
    "    level=logging.INFO,\n",
    "    format='%(message)s',  # Only the message, no timestamp etc.\n",
    "    filemode='w'  # Overwrite each run\n",
    ")\n",
    "logging.getLogger().setLevel(logging.WARNING)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ac98ff",
   "metadata": {},
   "source": [
    "## X-Coordinate Indentation Mapping\n",
    "\n",
    "Maps X-coordinate positions from the PDF to markdown indentation levels:\n",
    "- **Rule 1**: Indentation is strictly based on X thresholds (0/2/4/6 spaces)\n",
    "- **Rule 2**: Nested levels must have at least +2 more spaces than parent level\n",
    "\n",
    "Each range tuple: `(x_min, x_max, indent_spaces, bullet_marker, is_bullet)`\n",
    "- x_min, x_max: X-coordinate boundaries (in points)\n",
    "- indent_spaces: Number of spaces to apply (0, 2, 4, or 6)\n",
    "- bullet_marker: Expected bullet character (•, ◦, or None)\n",
    "- is_bullet: Whether this range is for bullets\n",
    "\n",
    "This ensures consistent hierarchical indentation across all content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "018183a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_indent_from_x(x_pos, bullet_char=None):\n",
    "    \"\"\"\n",
    "    Determine indentation level from X coordinate.\n",
    "    \n",
    "    Indentation is determined STRICTLY by X coordinate thresholds (0/2/4/6 spaces).\n",
    "    Nested levels must have at least +2 more spaces than parent level.\n",
    "    \n",
    "    Args:\n",
    "        x_pos: X coordinate from PDF (PRIMARY source of truth)\n",
    "        bullet_char: bullet character - currently unused, kept for API compatibility\n",
    "    \n",
    "    Returns:\n",
    "        indent_spaces: number of spaces for indentation (0, 2, 4, 6, etc.)\n",
    "    \"\"\"\n",
    "\n",
    "    # bullet_char parameter is ignored (kept for API compatibility)\n",
    "    # Check X coordinate ranges (PRIMARY source of truth)\n",
    "    logger.info(\"log_040: Determining indentation from X coordinate\")\n",
    "    for x_min, x_max, indent_spaces, bullet_marker, is_bullet in X_INDENT_RANGES:\n",
    "        if x_min <= x_pos < x_max:\n",
    "            logger.info(\"log_041: x_min <= x_pos < x_max\")\n",
    "            return indent_spaces\n",
    "    # Fallback: for x_pos >= 125, calculate additional indentation levels based on distance from 125\n",
    "    if x_pos >= 125:\n",
    "        logger.info(\"log_042: x_pos >= 125 fallback\")\n",
    "        extra_levels = max(0, int((x_pos - 125) / 20))\n",
    "        result = 4 + extra_levels * 2\n",
    "        return result\n",
    "    return 0  # Default: main level"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e91d7c",
   "metadata": {},
   "source": [
    "## PDF Block Processing: Merge Table Columns\n",
    "\n",
    "**merge_table_columns_in_block()**:\n",
    "- Merges table columns by grouping lines at similar Y coordinates within a PDF block\n",
    "- Special handling: Does NOT merge bullet characters with content text (keeps them separate)\n",
    "- Combines spans from multiple lines at the same Y position into single merged lines\n",
    "- Returns the modified block with merged lines for table processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "7e20682e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF Block Processing Functions\n",
    "\n",
    "def merge_table_columns_in_block(block):\n",
    "    \"\"\"\n",
    "    Merge table columns by grouping lines at similar Y coordinates.\n",
    "    Special handling: Do NOT merge bullet characters with content text.\n",
    "    \"\"\"\n",
    "    logger.info(\"log_042: Starting merge_table_columns_in_block\")\n",
    "    # Identify which lines contain ONLY a bullet character\n",
    "    pure_bullet_lines = set()\n",
    "    for line_idx, line in enumerate(block.get('lines', [])):\n",
    "        spans = line.get('spans', [])\n",
    "        if len(spans) == 1 and spans[0]['text'].strip() in BULLET_CHARS:\n",
    "            logger.info(f\"log_044: Found pure bullet line at index {line_idx}\")\n",
    "            pure_bullet_lines.add(line_idx)\n",
    "    # Collect all lines with their Y coordinate, excluding pure bullets from merge\n",
    "    y_groups = {}\n",
    "    for line_idx, line in enumerate(block.get('lines', [])):\n",
    "        # Don't merge pure bullet lines - keep them separate\n",
    "        if line_idx in pure_bullet_lines:\n",
    "            logger.info(f\"log_046: Line {line_idx} is pure bullet, separate group\")\n",
    "            y_groups[len(y_groups)] = [line]  # Each bullet gets its own group\n",
    "            continue\n",
    "        y_pos = line['bbox'][1]  # Top of bbox\n",
    "        # Find matching Y group (within tolerance), but only for non-bullet lines\n",
    "        matched_y = None\n",
    "        for existing_y in y_groups:\n",
    "            if isinstance(existing_y, (int, float)) and existing_y < 1000:\n",
    "                if abs(y_pos - existing_y) < Y_MERGE_TOLERANCE:\n",
    "                    logger.info(f\"log_047: Line {line_idx} matched Y group {existing_y}\")\n",
    "                    matched_y = existing_y\n",
    "                    break\n",
    "        if matched_y is None:\n",
    "            logger.info(f\"log_048: Line {line_idx} starts new Y group {y_pos}\")\n",
    "            matched_y = y_pos\n",
    "            y_groups[matched_y] = []\n",
    "        y_groups[matched_y].append(line)\n",
    "    # Sort each group by X coordinate and merge spans\n",
    "    merged_lines = []\n",
    "    for key in sorted(y_groups.keys(), key=lambda k: k if isinstance(k, (int, float)) else float('inf')):\n",
    "        lines_in_group = y_groups[key]\n",
    "        if len(lines_in_group) == 1:\n",
    "            logger.info(f\"log_049: Y group {key} has 1 line, no merge\")\n",
    "            merged_lines.append(lines_in_group[0])\n",
    "        else:\n",
    "            logger.info(f\"log_050: Y group {key} has {len(lines_in_group)} lines, merging\")\n",
    "            # Multiple lines at same Y - merge them\n",
    "            lines_sorted = sorted(lines_in_group, key=lambda l: l['bbox'][0])\n",
    "            # Combine spans from all lines, adding spaces between them\n",
    "            merged_spans = []\n",
    "            for line_idx, line in enumerate(lines_sorted):\n",
    "                for span_idx, span in enumerate(line['spans']):\n",
    "                    span_copy = dict(span)\n",
    "                    if line_idx > 0 or span_idx > 0:\n",
    "                        span_copy['text'] = ' ' + span['text']\n",
    "                    merged_spans.append(span_copy)\n",
    "            # Create merged line\n",
    "            merged_line = {\n",
    "                'spans': merged_spans,\n",
    "                'wmode': lines_sorted[0].get('wmode', 0),\n",
    "                'dir': lines_sorted[0].get('dir', [1.0, 0.0]),\n",
    "                'bbox': [\n",
    "                    lines_sorted[0]['bbox'][0],   # Left of first\n",
    "                    lines_sorted[0]['bbox'][1],   # Top of first\n",
    "                    lines_sorted[-1]['bbox'][2],  # Right of last\n",
    "                    lines_sorted[-1]['bbox'][3],  # Bottom of last\n",
    "                ]\n",
    "            }\n",
    "            merged_lines.append(merged_line)\n",
    "    return merged_lines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9ccca2",
   "metadata": {},
   "source": [
    "## PDF Block Processing: Identify Bullet Lines\n",
    "\n",
    "**identify_bullet_lines()**:\n",
    "- Identifies which lines in a merged block contain ONLY a bullet character (•, ◦, etc.)\n",
    "- Returns a set of line indices containing pure bullets\n",
    "- Used to properly handle bullets separately from their content during processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "5acd1a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_bullet_lines(merged_block_lines):\n",
    "    \"\"\"Identify which lines contain ONLY a bullet character.\"\"\"\n",
    "    logger.info(\"log_051: Starting identify_bullet_lines\")\n",
    "    bullet_lines = set()\n",
    "    for line_idx, line in enumerate(merged_block_lines):\n",
    "        spans = line.get('spans', [])\n",
    "        if len(spans) == 1 and spans[0]['text'].strip() in BULLET_CHARS:\n",
    "            logger.info(f\"log_052: Found pure bullet line at index {line_idx}\")\n",
    "            bullet_lines.add(line_idx)\n",
    "    return bullet_lines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37d2208",
   "metadata": {},
   "source": [
    "## Line Info Extraction & Helper Functions\n",
    "\n",
    "**extract_line_info()**:\n",
    "- Extracts metadata from each line: text, font size, flags, font name, X position, bullet info\n",
    "- Skips noise: headers, footers, font sizes 24.0 or 8.0\n",
    "- Detects if line had a bullet in previous line and captures bullet character\n",
    "- Returns list of dicts with line info for processing\n",
    "\n",
    "**should_close_code_block()**: Detects headings that should close open code blocks\n",
    "**get_heading_level()**: Determines markdown heading level (2-4) from font characteristics  \n",
    "**is_code_block_text()**: Checks if line is in code font (Consolas) at correct size (10.5pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "c8573ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_line_info(merged_block_lines, bullet_lines, page_num_1idx):\n",
    "    \"\"\"\n",
    "    Extract and process line metadata from merged block lines.\n",
    "    \n",
    "    Returns:\n",
    "        List of dicts with: text, size, flags, font, is_bold, x_pos, has_bullet, bullet_char, bullet_x_pos\n",
    "    \"\"\"\n",
    "    logger.info(\"log_060: Starting extract_line_info\")\n",
    "    lines_info = []\n",
    "    for line_idx, line in enumerate(merged_block_lines):\n",
    "        spans = line.get('spans', [])\n",
    "\n",
    "        # SKIP pure bullet lines - they only serve to mark the next line\n",
    "        if line_idx in bullet_lines:\n",
    "            logger.info(f\"log_054: Line {line_idx} is pure bullet, skipping\")\n",
    "            continue\n",
    "        first_span = spans[0]\n",
    "        span_size = first_span.get('size', 10)\n",
    "        line_flags = first_span.get('flags', 0)\n",
    "        line_font = first_span.get('font', 'Arial')\n",
    "        x_pos = first_span.get('origin', [0, 0])[0]\n",
    "        # Skip certain font sizes (headers, noise)\n",
    "        if round(span_size, 1) in [24.0, 8.0]:\n",
    "            logger.info(f\"log_055: Line {line_idx} has header/noise font size, skipping\")\n",
    "            continue\n",
    "        line_text = \"\".join([s['text'] for s in spans])\n",
    "        line_text = clean_page_noise(line_text, page_num_1idx).strip()\n",
    "        line_text_clean = line_text.lstrip(' ')\n",
    "        # Skip chapter/appendix headers and noise\n",
    "        if re.match(r'^(\\d{1,2}|[A-E]):\\s+\\S', line_text_clean):\n",
    "            logger.info(f\"log_056: Line {line_idx} matches chapter/appendix header, skipping\")\n",
    "            continue\n",
    "        if not line_text_clean or re.fullmatch(r'[\\.\\-\\(\\)\\s]*', line_text_clean):\n",
    "            logger.info(f\"log_057: Line {line_idx} is empty/noise, skipping\")\n",
    "            continue\n",
    "        if line_text_clean.strip() in [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\"]:\n",
    "            logger.info(f\"log_058: Line {line_idx} is page number, skipping\")\n",
    "            continue\n",
    "        is_bold_heading = (line_flags & BOLD_FLAG)\n",
    "        # Determine if this line had a bullet in previous line\n",
    "        has_bullet_from_previous = False\n",
    "        bullet_char = None\n",
    "        bullet_x_pos = None\n",
    "        if line_idx > 0 and (line_idx - 1) in bullet_lines:\n",
    "            logger.info(f\"log_059: Line {line_idx} has bullet from previous line\")\n",
    "            has_bullet_from_previous = True\n",
    "            prev_line = merged_block_lines[line_idx - 1]\n",
    "            if prev_line.get('spans'):\n",
    "                bullet_text = prev_line['spans'][0]['text'].strip()\n",
    "                bullet_char = bullet_text\n",
    "                bullet_x_pos = prev_line['spans'][0].get('origin', [0, 0])[0]\n",
    "        lines_info.append({\n",
    "            'text': line_text_clean,\n",
    "            'size': span_size,\n",
    "            'flags': line_flags,\n",
    "            'font': line_font,\n",
    "            'is_bold': is_bold_heading,\n",
    "            'x_pos': x_pos,\n",
    "            'has_bullet': has_bullet_from_previous,\n",
    "            'bullet_char': bullet_char,\n",
    "            'bullet_x_pos': bullet_x_pos,\n",
    "        })\n",
    "    return lines_info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8c5841",
   "metadata": {},
   "source": [
    "## PDF Block Processing Functions\n",
    "\n",
    "**merge_table_columns_in_block()**:\n",
    "- Merges table columns by grouping lines at similar Y coordinates\n",
    "- Preserves bullet characters on separate lines (not merged with content)\n",
    "- Handles multi-column layouts in the PDF\n",
    "\n",
    "**identify_bullet_lines()**:\n",
    "- Identifies which lines contain ONLY a bullet character (•, ◦, etc.)\n",
    "- Returns a set of line indices containing pure bullets\n",
    "- Used to properly handle bullets separately from their content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "095a4fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_close_code_block(line_info):\n",
    "    \"\"\"Determine if we should close any open code block for this line (heading detection).\"\"\"\n",
    "    logger.info(\"log_060: Starting should_close_code_block\")\n",
    "    span_size = line_info['size']\n",
    "    line_flags = line_info['flags']\n",
    "    line_font = line_info['font']\n",
    "    is_bold_heading = line_info['is_bold']\n",
    "    # Check if this is a heading that should close code blocks\n",
    "    if abs(span_size - H2_SIZE_SPECIFIC) < TOLERANCE and is_bold_heading:\n",
    "        logger.info(\"log_061: Detected H2 heading\")\n",
    "        return True\n",
    "    elif abs(span_size - H3_SIZE_GENERIC) < TOLERANCE and is_bold_heading:\n",
    "        logger.info(\"log_062: Detected H3 heading\")\n",
    "        return True\n",
    "    elif H4_FONT in line_font and abs(span_size - H4_SIZE_ARIAL_BLACK) < TOLERANCE:\n",
    "        logger.info(\"log_063: Detected H4 heading (Arial Black)\")\n",
    "        return True\n",
    "    elif round(span_size, 3) == 13.965 and is_bold_heading:\n",
    "        logger.info(\"log_064: Detected H4 heading (size 13.965)\")\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def get_heading_level(line_info):\n",
    "    \"\"\"Get markdown heading level if line is a heading, else None.\"\"\"\n",
    "    logger.info(\"log_065: Starting get_heading_level\")\n",
    "    span_size = line_info['size']\n",
    "    line_flags = line_info['flags']\n",
    "    line_font = line_info['font']\n",
    "    is_bold_heading = line_info['is_bold']\n",
    "    if abs(span_size - H2_SIZE_SPECIFIC) < TOLERANCE and is_bold_heading:\n",
    "        logger.info(\"log_066: Heading level 2 detected\")\n",
    "        return 2\n",
    "    elif abs(span_size - H3_SIZE_GENERIC) < TOLERANCE and is_bold_heading:\n",
    "        logger.info(\"log_067: Heading level 3 detected\")\n",
    "        return 3\n",
    "    elif H4_FONT in line_font and abs(span_size - H4_SIZE_ARIAL_BLACK) < TOLERANCE:\n",
    "        logger.info(\"log_068: Heading level 4 detected (Arial Black)\")\n",
    "        return 4\n",
    "    elif round(span_size, 3) == 13.965 and is_bold_heading:\n",
    "        logger.info(\"log_069: Heading level 4 detected (size 13.965)\")\n",
    "        return 4\n",
    "    return None\n",
    "\n",
    "def is_code_block_text(line_info):\n",
    "    \"\"\"Check if line is code text (code font + correct size).\"\"\"\n",
    "    logger.info(\"log_070: Starting is_code_block_text\")\n",
    "    return abs(line_info['size'] - BODY_TEXT_SIZE) < TOLERANCE and line_info['font'] == CODE_FONT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05be9e08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b84b78f8",
   "metadata": {},
   "source": [
    "## Core Page Parsing Logic\n",
    "\n",
    "**parse_page_with_metadata()**:\n",
    "- Main function that parses a single PDF page\n",
    "- Handles multi-page code block continuity via state tracking\n",
    "- For each line, determines:\n",
    "  1. Is it a heading? → Close code block, add markdown heading\n",
    "  2. Is it code? → Add to code block with proper formatting\n",
    "  3. Is it regular text? → Apply hierarchical indentation based on X coordinate\n",
    "\n",
    "**Key features**:\n",
    "- Tracks `in_code_block` state across pages\n",
    "- Applies indentation strictly from X_INDENT_RANGES\n",
    "- Handles numbered items and bullet markers\n",
    "- Returns markdown text and updated state for next page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "a63df0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_page_with_metadata(page, page_num_1idx, previous_page_state=None):\n",
    "    \"\"\"\n",
    "    Parse a page with table detection, code vs output distinction, and hierarchical indentation.\n",
    "    \n",
    "    FIX: Changed to insert tables at their correct block positions instead of all at once.\n",
    "    \n",
    "    Args:\n",
    "        page: PyMuPDF page object\n",
    "        page_num_1idx: 1-indexed page number\n",
    "        previous_page_state: dict tracking state from previous page\n",
    "    \n",
    "    Returns:\n",
    "        dict with 'markdown': str and 'state': dict for next page\n",
    "    \"\"\"\n",
    "    logger.info(f\"log_018: {page_num_1idx:04d} Starting parse_page_with_metadata\")\n",
    "    \n",
    "    data = page.get_text(\"dict\")\n",
    "    logger.info(f\"log_020: {page_num_1idx:04d} Got text dict\")\n",
    "    blocks = data['blocks']\n",
    "    logger.info(f\"log_021: {page_num_1idx:04d} Extracted {len(blocks)} blocks\")\n",
    "    markdown_lines = []\n",
    "    pending_numbered_item = None\n",
    "    \n",
    "    in_code_block = previous_page_state['in_code_block']\n",
    "    \n",
    "    # STEP 1: Prepare raw blocks for table detection\n",
    "    all_blocks_with_raw_lines = []\n",
    "    all_merged_blocks = {}\n",
    "    \n",
    "    for block_idx, block in enumerate(blocks):\n",
    "        if block['type'] != 0:\n",
    "            continue\n",
    "        \n",
    "        raw_lines = block.get('lines', [])\n",
    "        all_blocks_with_raw_lines.append((block_idx, raw_lines))\n",
    "        \n",
    "        merged_block_lines = merge_table_columns_in_block(block)\n",
    "        all_merged_blocks[block_idx] = merged_block_lines\n",
    "    \n",
    "    logger.info(f\"log_023: {page_num_1idx:04d} Prepared {len(all_blocks_with_raw_lines)} blocks for table detection\")\n",
    "\n",
    "    # STEP 2: Detect and convert tables\n",
    "    table_list, table_line_refs = detect_and_convert_tables_in_page(all_blocks_with_raw_lines, page_num_1idx)\n",
    "    logger.info(f\"log_024: {page_num_1idx:04d} Detected and converted tables\")\n",
    "\n",
    "    # Build dict mapping block_idx to its table markdown\n",
    "    # table_dict[block_idx] = markdown string to insert at this block\n",
    "    table_dict = {}  # block_idx -> markdown_str\n",
    "    for start_block, end_block, table_markdown in table_list:\n",
    "        # Insert table at the START block of this range\n",
    "        table_dict[start_block] = table_markdown\n",
    "    \n",
    "    # Build set of blocks that contain tables (from table_line_refs)\n",
    "    table_blocks_set = set(b_idx for b_idx, _ in table_line_refs)\n",
    "    logger.info(f\"log_025: {page_num_1idx:04d} Table blocks set: {len(table_blocks_set)}\")\n",
    "\n",
    "    # STEP 3: Process each block in order, inserting tables at their correct positions\n",
    "    for block_idx, block in enumerate(blocks):\n",
    "        if block['type'] != 0:\n",
    "            continue\n",
    "        \n",
    "        if block_idx not in all_merged_blocks:\n",
    "            continue\n",
    "        \n",
    "        # FIX: Insert table at this specific block position (if it's the start of a table range)\n",
    "        if block_idx in table_dict:\n",
    "            if in_code_block:\n",
    "                logger.info(f\"log_026: {page_num_1idx:04d} Closing code block for table\")\n",
    "                markdown_lines.append(\"```\\n\")\n",
    "                in_code_block = False\n",
    "            logger.info(f\"log_027: {page_num_1idx:04d} Inserting table at block {block_idx}\")\n",
    "            markdown_lines.append(table_dict[block_idx])\n",
    "            markdown_lines.append('\\n\\n')\n",
    "            # Skip all blocks that are part of this table\n",
    "            continue\n",
    "        \n",
    "        # Skip remaining table blocks (they're part of a table already inserted)\n",
    "        if block_idx in table_blocks_set:\n",
    "            continue\n",
    "        \n",
    "        merged_block_lines = all_merged_blocks[block_idx]\n",
    "        bullet_lines = identify_bullet_lines(merged_block_lines)\n",
    "        lines_info = extract_line_info(merged_block_lines, bullet_lines, page_num_1idx)\n",
    "        \n",
    "        # Process each line in the (possibly combined) block\n",
    "        for current in lines_info:\n",
    "            line_text_clean = current['text']\n",
    "            \n",
    "            # CONSOLIDATED: Check if heading (closes code blocks automatically)\n",
    "            if should_close_code_block(current):\n",
    "                if in_code_block:\n",
    "                    logger.info(f\"log_030: {page_num_1idx:04d} Closing code block for heading\")\n",
    "                    markdown_lines.append(\"```\\n\")\n",
    "                    in_code_block = False\n",
    "                \n",
    "                heading_level = get_heading_level(current)\n",
    "                heading_marker = '#' * heading_level\n",
    "                logger.info(f\"log_031: {page_num_1idx:04d} Adding heading\")\n",
    "                markdown_lines.append(f\"{heading_marker} {line_text_clean}\\n\")\n",
    "                continue\n",
    "            \n",
    "            # Code/Output handling\n",
    "            if is_code_block_text(current):\n",
    "                is_code = is_code_line(line_text_clean, current['font'])\n",
    "                \n",
    "                if is_code:\n",
    "                    # Start or continue python code block with REPL prompt\n",
    "                    if not in_code_block:\n",
    "                        logger.info(f\"log_032: {page_num_1idx:04d} Starting code block\")\n",
    "                        markdown_lines.append(\"```python\\n\")\n",
    "                        in_code_block = True\n",
    "                    logger.info(f\"log_033: {page_num_1idx:04d} Adding code line\")\n",
    "                    markdown_lines.append(line_text_clean + \"\\n\")\n",
    "                else:\n",
    "                    # This is output (code font text after >>> lines)\n",
    "                    if in_code_block:\n",
    "                        logger.info(f\"log_034: {page_num_1idx:04d} Adding output in code block\")\n",
    "                        markdown_lines.append(line_text_clean + \"\\n\")\n",
    "                    else:\n",
    "                        # Orphaned code-font text - treat as regular code\n",
    "                        if not in_code_block:\n",
    "                            logger.info(f\"log_035: {page_num_1idx:04d} Starting code block for output\")\n",
    "                            markdown_lines.append(\"```python\\n\")\n",
    "                            in_code_block = True\n",
    "                        markdown_lines.append(line_text_clean + \"\\n\")\n",
    "            \n",
    "            else:\n",
    "                # Regular body text (non-code-font) - apply hierarchical indentation\n",
    "                if in_code_block:\n",
    "                    logger.info(f\"log_036: {page_num_1idx:04d} Closing code block for body\")\n",
    "                    markdown_lines.append(\"```\\n\")\n",
    "                    in_code_block = False\n",
    "                \n",
    "                # Determine indentation from X coordinate (strictly based on X_INDENT_RANGES)\n",
    "                indent_x_pos = current['bullet_x_pos'] if current['has_bullet'] and current['bullet_x_pos'] is not None else current['x_pos']\n",
    "                indent_spaces = get_indent_from_x(indent_x_pos, current['bullet_char'])\n",
    "                \n",
    "                indent_str = ' ' * indent_spaces\n",
    "                \n",
    "                # Apply bullet marker if present\n",
    "                if current['has_bullet']:\n",
    "                    logger.info(f\"log_037: {page_num_1idx:04d} Adding bullet line\")\n",
    "                    markdown_lines.append(f\"{indent_str}* {line_text_clean}\\n\")\n",
    "                else:\n",
    "                    logger.info(f\"log_038: {page_num_1idx:04d} Adding body line\")\n",
    "                    markdown_lines.append(f\"{indent_str}{line_text_clean}\\n\")\n",
    "    \n",
    "    # Determine final state for next page\n",
    "    final_state = {\n",
    "        'in_code_block': in_code_block,\n",
    "        'pending_lines': []\n",
    "    }\n",
    "    \n",
    "    final_text = \"\".join(markdown_lines)\n",
    "    final_text = re.sub(r'\\n{3,}', '\\n\\n', final_text)\n",
    "    logger.info(f\"log_039: {page_num_1idx:04d} Finalized markdown\")\n",
    "    \n",
    "    return {\n",
    "        'markdown': final_text.strip(),\n",
    "        'state': final_state\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92bd012",
   "metadata": {},
   "source": [
    "## Bullet Conversion & File Export Functions\n",
    "\n",
    "**convert_bullets_in_text()**:\n",
    "- Post-processing step to convert old bullet characters to markdown format\n",
    "- Replaces • and ◦ with * (markdown bullet syntax)\n",
    "- Preserves indentation from X coordinate parsing\n",
    "- Ensures proper spacing for nested bullets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "d7c4a92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bullet Conversion & File Export Functions\n",
    "\n",
    "def convert_bullets_in_text(text):\n",
    "    \"\"\"\n",
    "    Convert old bullet characters (• and ◦) to markdown format (* with proper indentation).\n",
    "    \n",
    "    This is a post-processing step for any bullets that weren't converted during parsing.\n",
    "    \"\"\"\n",
    "    logger.info(\"log_071: Starting convert_bullets_in_text\")\n",
    "    lines = text.split('\\n')\n",
    "    converted_lines = []\n",
    "    for line in lines:\n",
    "        # Replace • with * (main bullet)\n",
    "        if '•' in line or '\\u2022' in line:\n",
    "            logger.info(\"log_072: Found main bullet line\")\n",
    "            match = re.match(r'^(\\s*)([•\\u2022])\\s*(.*)', line)\n",
    "            if match:\n",
    "                logger.info(\"log_073: Main bullet line matches regex\")\n",
    "                spaces, bullet, content = match.groups()\n",
    "                converted_lines.append(f\"{spaces}* {content}\")\n",
    "            else:\n",
    "                logger.info(\"log_074: Main bullet line does not match regex, replacing directly\")\n",
    "                line = line.replace('•', '*').replace('\\u2022', '*')\n",
    "                converted_lines.append(line)\n",
    "        # Replace ◦ with * (nested bullet) - ensure 4 spaces\n",
    "        elif '◦' in line or '\\u25E6' in line:\n",
    "            logger.info(\"log_075: Found nested bullet line\")\n",
    "            match = re.match(r'^(\\s*)([◦\\u25E6])\\s*(.*)', line)\n",
    "            if match:\n",
    "                logger.info(\"log_076: Nested bullet line matches regex\")\n",
    "                spaces, bullet, content = match.groups()\n",
    "                converted_lines.append(f\"{spaces}* {content}\")\n",
    "            else:\n",
    "                logger.info(\"log_078: Nested bullet line does not match regex, replacing directly\")\n",
    "                line = line.replace('◦', '*').replace('\\u25E6', '*')\n",
    "                converted_lines.append(line)\n",
    "        else:\n",
    "            converted_lines.append(line)\n",
    "    return '\\n'.join(converted_lines)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530622f0",
   "metadata": {},
   "source": [
    "## Detect Table Blocks in Page\n",
    "\n",
    "**detect_table_blocks_in_page()**:\n",
    "- Detects table structures by identifying header blocks and their associated data blocks\n",
    "- Uses font characteristics and X-coordinate alignment to identify table components\n",
    "- Returns ranges of blocks that contain tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "61a9c8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_table_blocks_in_page(all_blocks_with_raw_lines):\n",
    "    \"\"\"\n",
    "    Detect table blocks by identifying header blocks and grouping all data blocks\n",
    "    that fall within the header's column threshold range.\n",
    "    \n",
    "    Key insight: A table consists of:\n",
    "    1. A header block with 2+ distinct columns (50px+ gaps)\n",
    "    2. All consecutive data blocks whose X coordinates fall within the header's range\n",
    "    3. Stops at footer or header blocks\n",
    "    4. ONLY the FIRST occurrence of a multi-column block in a given X range is treated as a header\n",
    "       This avoids treating data rows (Block 1 with 2 columns) as separate headers\n",
    "    \n",
    "    For multi-block-per-row tables (like page 10):\n",
    "    - Header (Block 0): X=[70, 130, 180] defines column thresholds\n",
    "    - Block 1: X=[70, 130] → within header range ✓\n",
    "    - Block 2: X=[180] → within header range ✓\n",
    "    - Block 3: X=[190+] → outside range (separate table or plain text)\n",
    "    \n",
    "    Args:\n",
    "        all_blocks_with_raw_lines: List of (block_idx, raw_lines) tuples\n",
    "    \n",
    "    Returns:\n",
    "        List of (table_start_block, table_end_block) tuples for detected tables\n",
    "    \"\"\"\n",
    "    logger.info(\"log_079: Starting detect_table_blocks_in_page\")\n",
    "    # Regex to detect footer/header text (version-agnostic)\n",
    "    FOOTER_PATTERN = re.compile(r'Package for Python User Guide,?\\s+Release\\s+[\\d.]+', re.IGNORECASE)\n",
    "    TABLE_FONT_SIZE = 10.028  # Table text font size (with tolerance)\n",
    "    FONT_SIZE_TOLERANCE = 0.05\n",
    "    MIN_X_GAP = 30  # Minimum gap between columns (distinct columns)\n",
    "    X_THRESHOLD_TOLERANCE = 100  # Allow 100px tolerance when checking if block X is within header range\n",
    "    # Step 1: Find all blocks with table-like font characteristics\n",
    "    block_characteristics = {}  # Store characteristics for each block\n",
    "    header_candidates = []  # Blocks with 2+ columns and 50px+ gaps\n",
    "    for block_idx, raw_lines in all_blocks_with_raw_lines:\n",
    "        # Check if this block has table-like characteristics\n",
    "        has_table_font = False\n",
    "        x_positions = set()\n",
    "        x_min, x_max = float('inf'), 0\n",
    "        for line in raw_lines:\n",
    "            for span in line.get('spans', []):\n",
    "                # Filter out footer text via regex\n",
    "                span_text = span.get('text', '')\n",
    "                if FOOTER_PATTERN.search(span_text):\n",
    "                    logger.info(f\"log_081: Block {block_idx} span matches footer pattern, skipping\")\n",
    "                    continue  # Skip footer lines\n",
    "                # Check font size\n",
    "                span_size = span.get('size', 0)\n",
    "                if abs(span_size - TABLE_FONT_SIZE) < FONT_SIZE_TOLERANCE:\n",
    "                    logger.info(f\"log_082: Block {block_idx} span matches table font size\")\n",
    "                    has_table_font = True\n",
    "                    x = span.get('origin', [0, 0])[0]\n",
    "                    x_key = x  # Use exact X value, do not round\n",
    "                    x_positions.add(x_key)\n",
    "                    x_min = min(x_min, x_key)\n",
    "                    x_max = max(x_max, x_key)\n",
    "        # Store characteristics for this block\n",
    "        block_characteristics[block_idx] = {\n",
    "            'has_table_font': has_table_font,\n",
    "            'x_positions': x_positions,\n",
    "            'x_min': x_min if x_min != float('inf') else 0,\n",
    "            'x_max': x_max,\n",
    "            'num_columns': len(x_positions)\n",
    "        }\n",
    "        # Identify header candidate blocks: have 2+ X positions with distinct columns (gap >= MIN_X_GAP)\n",
    "        # BUT REJECT blocks with >5 unique X positions (likely wrapped text, not a table)\n",
    "        if has_table_font and len(x_positions) >= 2 and len(x_positions) <= 5:\n",
    "            sorted_x = sorted(x_positions)\n",
    "            for i in range(len(sorted_x) - 1):\n",
    "                if sorted_x[i+1] - sorted_x[i] >= MIN_X_GAP:\n",
    "                    logger.info(f\"log_083: Block {block_idx} is header candidate\")\n",
    "                    header_candidates.append(block_idx)\n",
    "                    break\n",
    "    # Pre-compute block colors and Y positions to avoid repeated iterations\n",
    "    block_color_map = {}\n",
    "    block_min_y_map = {}\n",
    "    for block_idx, raw_lines in all_blocks_with_raw_lines:\n",
    "        # Extract color\n",
    "        color = None\n",
    "        for line in raw_lines:\n",
    "            for span in line.get('spans', []):\n",
    "                color = span.get('color')\n",
    "                if color is not None:\n",
    "                    break\n",
    "            if color is not None:\n",
    "                break\n",
    "        block_color_map[block_idx] = color\n",
    "        # Extract min Y\n",
    "        min_y = float('inf')\n",
    "        for line in raw_lines:\n",
    "            for span in line.get('spans', []):\n",
    "                y = span.get('origin', [0, 0])[1]\n",
    "                min_y = min(min_y, y)\n",
    "        block_min_y_map[block_idx] = min_y if min_y != float('inf') else None\n",
    "    # Step 2: Filter to ONLY keep headers (not data rows that look like headers)\n",
    "    # If two header candidates have overlapping X ranges, only the FIRST is a header\n",
    "    if not header_candidates:\n",
    "        logger.info(\"log_084: No header candidates found\")\n",
    "        return []\n",
    "    real_headers = []\n",
    "    used_x_ranges = []\n",
    "    for header_idx in sorted(header_candidates):\n",
    "        # Check for non-table blocks between the last real header and this candidate\n",
    "        # If a non-table block exists, it's a separator, so we reset the X-range memory\n",
    "        last_header = real_headers[-1] if real_headers else -1\n",
    "        has_separator = False\n",
    "        for i in range(last_header + 1, header_idx):\n",
    "            if i in block_characteristics and not block_characteristics[i]['has_table_font']:\n",
    "                logger.info(f\"log_085: Separator found between headers at {i}\")\n",
    "                has_separator = True\n",
    "                break\n",
    "        if has_separator:\n",
    "            # A non-table block was found, so this is a new section. Reset used ranges.\n",
    "            logger.info(f\"log_086: Resetting used_x_ranges due to separator before header {header_idx}\")\n",
    "            used_x_ranges = []\n",
    "        header_chars = block_characteristics[header_idx]\n",
    "        header_x_positions = header_chars['x_positions']\n",
    "        header_x_min = min(header_x_positions) if header_x_positions else 0\n",
    "        header_x_max = max(header_x_positions) if header_x_positions else 0\n",
    "        # Check if this X range overlaps with any existing header's X range\n",
    "        is_overlapping = False\n",
    "        for existing_min, existing_max in used_x_ranges:\n",
    "            # Check if X ranges overlap significantly\n",
    "            if (header_x_min <= existing_max + X_THRESHOLD_TOLERANCE and \n",
    "                header_x_max >= existing_min - X_THRESHOLD_TOLERANCE):\n",
    "                logger.info(f\"log_087: Header {header_idx} X range overlaps with previous header\")\n",
    "                is_overlapping = True\n",
    "                break\n",
    "        if not is_overlapping:\n",
    "            # This is a truly new header (different table)\n",
    "            logger.info(f\"log_088: Header {header_idx} is a real header\")\n",
    "            real_headers.append(header_idx)\n",
    "            used_x_ranges.append((header_x_min, header_x_max))\n",
    "    # Step 2B: Extend header ranges to include ALL previous blocks with same color\n",
    "    # This handles cases where table headers span multiple blocks with varying Y positions\n",
    "    # (e.g., page 203 blocks 14-15-16, where 14-15 have same color but different Y than 16)\n",
    "    # Track: (original_header_idx) -> (extended_start_idx, extended_end_idx)\n",
    "    header_extension_map = {}  # Maps original header to (start, end) of extended range\n",
    "    blocks_to_skip = set()  # Blocks that were absorbed into extended headers\n",
    "    for header_idx in real_headers:\n",
    "        header_start_idx = header_idx\n",
    "        header_end_idx = header_idx\n",
    "        header_color = block_color_map[header_idx]\n",
    "        # Look backward to include ALL consecutive blocks with same color\n",
    "        # This ensures multi-block headers are fully captured even if blocks have different Y positions\n",
    "        check_idx = header_idx - 1\n",
    "        while check_idx >= 0:\n",
    "            # Check if previous block has table font\n",
    "            if check_idx not in block_characteristics or not block_characteristics[check_idx]['has_table_font']:\n",
    "                break\n",
    "            # Check if previous block has same color\n",
    "            prev_color = block_color_map.get(check_idx)\n",
    "            if prev_color == header_color:\n",
    "                logger.info(f\"log_089: Extending header {header_idx} backward to {check_idx}\")\n",
    "                header_start_idx = check_idx\n",
    "                blocks_to_skip.add(check_idx)\n",
    "                check_idx -= 1\n",
    "            else:\n",
    "                # Color changed, stop looking backward\n",
    "                break\n",
    "        # Store the extended range\n",
    "        header_extension_map[header_idx] = (header_start_idx, header_end_idx)\n",
    "    # Step 3: Group blocks into tables starting from each real header block\n",
    "    table_ranges = []\n",
    "    processed_blocks = set()\n",
    "    for header_idx in sorted(real_headers):\n",
    "\n",
    "        # Get extended range for this header\n",
    "        header_start_idx, header_end_idx = header_extension_map[header_idx]\n",
    "        # Recompute header X positions from ALL blocks in extended range\n",
    "        header_x_positions = set()\n",
    "        for h_idx in range(header_start_idx, header_end_idx + 1):\n",
    "            if h_idx in block_characteristics:\n",
    "                header_x_positions.update(block_characteristics[h_idx]['x_positions'])\n",
    "\n",
    "        header_x_min = min(header_x_positions)\n",
    "        header_x_max = max(header_x_positions)\n",
    "        # Mark all blocks in extended header range as processed\n",
    "        for h_idx in range(header_start_idx, header_end_idx + 1):\n",
    "            processed_blocks.add(h_idx)\n",
    "        # Store the extended header as a single table range\n",
    "        logger.info(f\"log_092: Storing extended header range {header_start_idx}-{header_end_idx}\")\n",
    "        table_ranges.append((header_start_idx, header_end_idx))\n",
    "        # Now look for consecutive DATA blocks that belong to this table\n",
    "        # These can have continuation blocks merged (blocks with no column 1 content)\n",
    "        data_range_start = None\n",
    "        for candidate_idx in range(header_end_idx + 1, len(block_characteristics)):\n",
    "\n",
    "            candidate_chars = block_characteristics[candidate_idx]\n",
    "            # Skip blocks without table font (likely footer/header)\n",
    "            if not candidate_chars['has_table_font']:\n",
    "                logger.info(f\"log_094: Candidate block {candidate_idx} has no table font, stopping data range\")\n",
    "                # Save any pending data range before stopping\n",
    "                if data_range_start is not None:\n",
    "                    logger.info(f\"log_095: Saving data range {data_range_start}-{candidate_idx-1}\")\n",
    "                    table_ranges.append((data_range_start, candidate_idx - 1))\n",
    "                    data_range_start = None\n",
    "                break\n",
    "            # Check if candidate is another real header (different table)\n",
    "            if candidate_idx in real_headers:\n",
    "                logger.info(f\"log_096: Candidate block {candidate_idx} is another real header, stopping data range\")\n",
    "                # Save any pending data range and stop\n",
    "                if data_range_start is not None:\n",
    "                    logger.info(f\"log_097: Saving data range {data_range_start}-{candidate_idx-1}\")\n",
    "                    table_ranges.append((data_range_start, candidate_idx - 1))\n",
    "                    data_range_start = None\n",
    "                break\n",
    "            # Check if candidate block's X coordinates mean it belongs to table\n",
    "            candidate_x_positions = candidate_chars['x_positions']\n",
    "            if candidate_x_positions:  # Has at least one X position\n",
    "                candidate_x_min = min(candidate_x_positions)\n",
    "                # STOP if text extends significantly to the left of column 1\n",
    "                if candidate_x_min < header_x_min - X_THRESHOLD_TOLERANCE:\n",
    "                    logger.info(f\"log_098: Candidate block {candidate_idx} is outside table boundary, stopping data range\")\n",
    "                    # This block is outside the table boundary. Save any pending data range and stop.\n",
    "                    if data_range_start is not None:\n",
    "                        logger.info(f\"log_099: Saving data range {data_range_start}-{candidate_idx-1}\")\n",
    "                        table_ranges.append((data_range_start, candidate_idx - 1))\n",
    "                        data_range_start = None\n",
    "                    break\n",
    "                # CONTINUE: This block belongs to the table\n",
    "                if data_range_start is None:\n",
    "                    logger.info(f\"log_100: Starting new data range at {candidate_idx}\")\n",
    "                    data_range_start = candidate_idx\n",
    "                processed_blocks.add(candidate_idx)\n",
    "    return table_ranges\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace1e618",
   "metadata": {},
   "source": [
    "## Extract Table Rows from Blocks\n",
    "\n",
    "**extract_table_rows_from_blocks()**:\n",
    "- Extracts structured table rows from PDF blocks using column thresholds\n",
    "- Handles multi-block rows by merging blocks without column 1 content\n",
    "- Returns a list of rows with aligned cell data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "95a1e727",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_table_rows_from_blocks(all_blocks_with_raw_lines, table_start, table_end, header_thresholds=None):\n",
    "    \"\"\"\n",
    "    Extract table rows by aligning cells to column thresholds from the TITLE ROW or provided thresholds.\n",
    "    Merges blocks that have no content in column 1 with the previous block.\n",
    "    \n",
    "    Key Logic:\n",
    "    1. If header_thresholds provided, use those. Otherwise, title blocks (table_start to table_end) define thresholds\n",
    "    2. Column 1 = [min_threshold, threshold[1]), Column 2 = [threshold[1], threshold[2]), etc.\n",
    "    3. For each subsequent block:\n",
    "       - If it has NO content in column 1 range, merge it with previous block\n",
    "       - If it HAS content in column 1, start a new row\n",
    "    4. Initialize empty columns as \"\" when merging blocks\n",
    "    \n",
    "    Args:\n",
    "        all_blocks_with_raw_lines: List of (block_idx, raw_lines) tuples\n",
    "        table_start, table_end: Block range\n",
    "        header_thresholds: Optional pre-computed column thresholds (use when extracting data-only ranges)\n",
    "    \n",
    "    Returns: List of rows, each row is a list of cells (text strings)\n",
    "    \"\"\"\n",
    "    logger.info(\"log_104: Starting extract_table_rows_from_blocks\")\n",
    "    block_dict = dict(all_blocks_with_raw_lines)\n",
    "    # Step 1: Collect all spans with block_idx, Y, X, text\n",
    "    all_spans_with_coords = []\n",
    "    for block_idx in range(table_start, table_end + 1):\n",
    "        for line in block_dict[block_idx]:\n",
    "            for span in line.get('spans', []):\n",
    "                y = span.get('origin', [0, 0])[1]\n",
    "                x = span.get('origin', [0, 0])[0]\n",
    "                text = span.get('text', '')\n",
    "                all_spans_with_coords.append((block_idx, y, x, text))\n",
    "\n",
    "    # Step 2: Extract column thresholds from provided thresholds OR title blocks\n",
    "    if header_thresholds is not None:\n",
    "        logger.info(\"log_108: Using provided header_thresholds\")\n",
    "        column_thresholds = sorted(header_thresholds)\n",
    "    else:\n",
    "        # Extract thresholds from title block range (ALL blocks from table_start to table_end)\n",
    "        # This handles cases where headers span multiple blocks (e.g., page 86 blocks 11-12)\n",
    "        title_block_spans = [(y, x, text) for b, y, x, t in all_spans_with_coords if table_start <= b <= table_end]\n",
    "        if title_block_spans:\n",
    "            logger.info(\"log_109: Extracting thresholds from title block spans\")\n",
    "            x_raw = [x for y, x, text in title_block_spans]\n",
    "            # Cluster X positions within 30px as same column\n",
    "            # Use actual X values (not rounded) to preserve precision for range checks\n",
    "            clustered_x = []\n",
    "            for x_val in sorted(set(x_raw)):\n",
    "                if not clustered_x or abs(x_val - clustered_x[-1]) > 30:\n",
    "                    clustered_x.append(x_val)\n",
    "            column_thresholds = sorted(clustered_x)\n",
    "\n",
    "\n",
    "    # Step 3: Helper function to assign X coordinate to column index\n",
    "    def assign_to_column_idx(x_coord):\n",
    "        \"\"\"\n",
    "        Assign X coordinate to column index using nearest-column logic.\n",
    "        For large column gaps (>100px), use distance to nearest column center.\n",
    "        For small gaps, use range-based assignment.\n",
    "        \"\"\"\n",
    "\n",
    "        # Check if this is a wide-gap table (columns >100px apart)\n",
    "        gaps = [column_thresholds[i+1] - column_thresholds[i] for i in range(len(column_thresholds)-1)]\n",
    "        has_wide_gap = any(gap > 100 for gap in gaps)\n",
    "        if has_wide_gap:\n",
    "            logger.info(\"log_114: Wide-gap table detected, using nearest-column assignment\")\n",
    "            nearest_idx = min(range(len(column_thresholds)), \n",
    "                             key=lambda i: abs(x_coord - column_thresholds[i]))\n",
    "            return nearest_idx\n",
    "        else:\n",
    "            # Use range-based assignment for narrow columns\n",
    "            for i in range(len(column_thresholds)):\n",
    "                if i == len(column_thresholds) - 1:\n",
    "                    # Last column: threshold[i] to infinity\n",
    "                    if x_coord >= column_thresholds[i]:\n",
    "                        logger.info(f\"log_115: Assigning x_coord {x_coord} to last column {i}\")\n",
    "                        return i\n",
    "                else:\n",
    "                    # Middle column: threshold[i] to threshold[i+1]\n",
    "                    if column_thresholds[i] <= x_coord < column_thresholds[i + 1]:\n",
    "                        logger.info(f\"log_116: Assigning x_coord {x_coord} to column {i}\")\n",
    "                        return i\n",
    "            # Fallback: assign to nearest threshold\n",
    "            logger.info(f\"log_117: Fallback assigning x_coord {x_coord} to nearest column\")\n",
    "            nearest_idx = min(range(len(column_thresholds)), \n",
    "                             key=lambda i: abs(x_coord - column_thresholds[i]))\n",
    "            return nearest_idx\n",
    "    # Step 4: Group spans by block first, then assign columns within each block\n",
    "    block_spans_dict = {}  # {block_idx: {col_idx: [text_list]}}\n",
    "    for block_idx, y, x, text in all_spans_with_coords:\n",
    "        col_idx = assign_to_column_idx(x)\n",
    "        if block_idx not in block_spans_dict:\n",
    "            block_spans_dict[block_idx] = {}\n",
    "        if col_idx not in block_spans_dict[block_idx]:\n",
    "            block_spans_dict[block_idx][col_idx] = []\n",
    "        block_spans_dict[block_idx][col_idx].append(text)\n",
    "    # Step 5: Combine text for each cell and identify blocks with no column 0 content\n",
    "    rows = []\n",
    "    current_row = None  # Will accumulate columns from merged blocks\n",
    "    current_row_cols = None  # Tracks which columns have been filled\n",
    "    for block_idx in sorted(block_spans_dict.keys()):\n",
    "        block_cols = block_spans_dict[block_idx]\n",
    "        # Check if this block has content in column 0 (first column)\n",
    "        has_col_0 = 0 in block_cols\n",
    "        if has_col_0:\n",
    "            logger.info(f\"log_118: Block {block_idx} has column 0, starting new row\")\n",
    "            # Start a new row\n",
    "            if current_row is not None:\n",
    "                logger.info(f\"log_119: Saving previous row for block {block_idx}\")\n",
    "                rows.append(current_row)\n",
    "            # Initialize new row with all columns\n",
    "            current_row = []\n",
    "            current_row_cols = set()\n",
    "            for col_idx in range(len(column_thresholds)):\n",
    "                if col_idx in block_cols:\n",
    "                    combined_text = ' '.join(block_cols[col_idx]).strip()\n",
    "                    current_row.append(combined_text)\n",
    "                    current_row_cols.add(col_idx)\n",
    "                else:\n",
    "                    current_row.append(\"\")\n",
    "        else:\n",
    "            logger.info(f\"log_120: Block {block_idx} has no column 0, merging with previous row\")\n",
    "            # No column 0 content: merge with previous row\n",
    "            if current_row is None:\n",
    "                logger.info(f\"log_121: No current row, initializing empty row for block {block_idx}\")\n",
    "                current_row = [\"\"] * len(column_thresholds)\n",
    "                current_row_cols = set()\n",
    "            # Merge block content into current row\n",
    "            for col_idx in range(len(column_thresholds)):\n",
    "                if col_idx in block_cols:\n",
    "                    combined_text = ' '.join(block_cols[col_idx]).strip()\n",
    "                    # Append to existing cell if already has content\n",
    "                    if current_row[col_idx]:\n",
    "                        logger.info(f\"log_122: Appending to existing cell in column {col_idx} for block {block_idx}\")\n",
    "                        current_row[col_idx] += \" \" + combined_text\n",
    "                    else:\n",
    "                        logger.info(f\"log_123: Setting cell in column {col_idx} for block {block_idx}\")\n",
    "                        current_row[col_idx] = combined_text\n",
    "                    current_row_cols.add(col_idx)\n",
    "    # Step 6: Add the last row\n",
    "    if current_row is not None and any(current_row):\n",
    "        logger.info(\"log_124: Adding final row\")\n",
    "        rows.append(current_row)\n",
    "    return rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa2dbed",
   "metadata": {},
   "source": [
    "## Format Markdown Table & Convert Tables\n",
    "\n",
    "**format_markdown_table()**:\n",
    "- Converts extracted table rows to markdown format (| separated columns)\n",
    "- Deduplicates consecutive identical rows (header repetition in PDFs)\n",
    "- Pads all rows to same column count with empty strings\n",
    "- Returns properly formatted markdown table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "a3ca536a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_markdown_table(rows):\n",
    "    \"\"\"\n",
    "    Convert extracted table rows to markdown table format.\n",
    "    \n",
    "    Handles:\n",
    "    1. Deduplication of consecutive identical rows (header repetition)\n",
    "    2. Proper column alignment\n",
    "    3. Markdown pipe separators\n",
    "    \n",
    "    Args:\n",
    "        rows: List of lists, each inner list is a row of cells\n",
    "    \n",
    "    Returns:\n",
    "        Formatted markdown table string\n",
    "    \"\"\"\n",
    "    logger.info(\"log_125: Starting format_markdown_table\")\n",
    "\n",
    "    def normalize_row(row):\n",
    "        \"\"\"Normalize row for deduplication: lowercase, collapse whitespace.\"\"\"\n",
    "        normalized_cells = [re.sub(r'\\s+', ' ', cell.strip().lower()) for cell in row]\n",
    "        return normalized_cells\n",
    "    # Deduplicate consecutive identical rows (header rows appearing multiple times)\n",
    "    deduplicated = []\n",
    "    prev_normalized = None\n",
    "    for row in rows:\n",
    "        curr_normalized = normalize_row(row)\n",
    "        if curr_normalized != prev_normalized:\n",
    "            deduplicated.append(row)\n",
    "            prev_normalized = curr_normalized\n",
    "    rows = deduplicated\n",
    "\n",
    "    # Determine column count\n",
    "    num_cols = max(len(row) for row in rows) if rows else 0\n",
    "    logger.info(f\"log_128: Number of columns detected: {num_cols}\")\n",
    "\n",
    "    # Ensure all rows have same column count\n",
    "    for row in rows:\n",
    "        while len(row) < num_cols:\n",
    "            row.append(\"\")\n",
    "    # Build markdown table\n",
    "    markdown_lines = []\n",
    "    # Header row\n",
    "    markdown_lines.append(\"| \" + \" | \".join(rows[0]) + \" |\")\n",
    "    # Separator row\n",
    "    separator = \"| \" + \" | \".join([\"-\" * max(1, len(cell)) for cell in rows[0]]) + \" |\"\n",
    "    markdown_lines.append(separator)\n",
    "    # Data rows\n",
    "    for i, row in enumerate(rows[1:], start=1):\n",
    "        logger.info(f\"log_130: Adding data row {i}\")\n",
    "        markdown_lines.append(\"| \" + \" | \".join(row) + \" |\")\n",
    "    return \"\\n\".join(markdown_lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f183ff29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "adfbda3a",
   "metadata": {},
   "source": [
    "## Detect and Convert Tables in Page\n",
    "\n",
    "**detect_and_convert_tables_in_page()**:\n",
    "- Orchestrates table detection and conversion to markdown\n",
    "- Combines header and data rows into properly formatted tables\n",
    "- Returns markdown table strings and references to processed blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "268ec126",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_and_convert_tables_in_page(all_blocks_with_raw_lines, page_num_1idx):\n",
    "    \"\"\"\n",
    "    Master function: Detect tables on page and convert to markdown format.\n",
    "    \n",
    "    FIX: Returns list of (start_block, end_block, markdown) tuples instead of combined markdown.\n",
    "    This allows parse_page_with_metadata() to insert tables at their correct block positions.\n",
    "    \n",
    "    Args:\n",
    "        all_blocks_with_raw_lines: List of (block_idx, raw_lines) tuples\n",
    "        page_num_1idx: Page number (1-indexed, for debugging)\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (table_list, set of (block_idx, line_idx) table references)\n",
    "        where table_list = [(start_block, end_block, markdown_str), ...]\n",
    "    \"\"\"\n",
    "    logger.info(f\"log_139: Starting detect_and_convert_tables_in_page for page {page_num_1idx}\")\n",
    "    # Detect table block ranges\n",
    "    table_ranges = detect_table_blocks_in_page(all_blocks_with_raw_lines)\n",
    "    logger.info(f\"log_140: Detected table ranges: {table_ranges}\")\n",
    "    if not table_ranges:\n",
    "        logger.info(\"log_141: No table ranges found, returning empty list and set\")\n",
    "        return [], set()\n",
    "    # Build a map of header ranges to their thresholds\n",
    "    header_thresholds_map = {}  # Maps header_end_idx -> thresholds\n",
    "    for idx, (table_start, table_end) in enumerate(table_ranges):\n",
    "        if idx + 1 < len(table_ranges):\n",
    "            next_start, next_end = table_ranges[idx + 1]\n",
    "            if next_start == table_end + 1:\n",
    "                rows = extract_table_rows_from_blocks(all_blocks_with_raw_lines, table_start, table_end)\n",
    "                if rows:\n",
    "                    block_dict = dict(all_blocks_with_raw_lines)\n",
    "                    x_positions = []\n",
    "                    for block_idx in range(table_start, table_end + 1):\n",
    "                        header_block_lines = block_dict.get(block_idx, [])\n",
    "                        for line in header_block_lines:\n",
    "                            for span in line.get('spans', []):\n",
    "                                x = span.get('origin', [0, 0])[0]\n",
    "                                x_positions.append(x)\n",
    "                    clustered_x = []\n",
    "                    for x_val in sorted(set(x_positions)):\n",
    "                        if not clustered_x or abs(x_val - clustered_x[-1]) > 30:\n",
    "                            clustered_x.append(x_val)\n",
    "                    header_thresholds_map[table_end] = sorted(clustered_x)\n",
    "    # Extract and convert each table range\n",
    "    all_table_markdown = []\n",
    "    all_table_refs = set()\n",
    "    table_ranges_with_idx = []  # Track (start, end, index_in_all_table_markdown)\n",
    "    i = 0\n",
    "    while i < len(table_ranges):\n",
    "        table_start, table_end = table_ranges[i]\n",
    "        logger.info(f\"log_142: Processing table range {table_start}-{table_end}\")\n",
    "        if table_end in header_thresholds_map and i + 1 < len(table_ranges):\n",
    "            next_start, next_end = table_ranges[i + 1]\n",
    "            if next_start == table_end + 1:\n",
    "                logger.info(f\"log_143: Header range {table_start}-{table_end} followed by data range {next_start}-{next_end}\")\n",
    "                header_thresholds = header_thresholds_map.get(table_end, None)\n",
    "                header_rows = extract_table_rows_from_blocks(all_blocks_with_raw_lines, table_start, table_end, header_thresholds)\n",
    "                data_rows = extract_table_rows_from_blocks(all_blocks_with_raw_lines, next_start, next_end, header_thresholds)\n",
    "                combined_rows = header_rows + data_rows\n",
    "                table_md = format_markdown_table(combined_rows)\n",
    "                if table_md:\n",
    "                    logger.info(f\"log_144: Table markdown generated for blocks {table_start}-{next_end}\")\n",
    "                    all_table_markdown.append(table_md)\n",
    "                    table_ranges_with_idx.append((table_start, next_end, len(all_table_markdown) - 1))\n",
    "                    for block_idx in range(table_start, next_end + 1):\n",
    "                        all_table_refs.add((block_idx, 0))\n",
    "                i += 2\n",
    "                continue\n",
    "        header_thresholds = None\n",
    "        rows = extract_table_rows_from_blocks(all_blocks_with_raw_lines, table_start, table_end, header_thresholds)\n",
    "        table_md = format_markdown_table(rows)\n",
    "        if table_md:\n",
    "            logger.info(f\"log_145: Table markdown generated for blocks {table_start}-{table_end}\")\n",
    "            all_table_markdown.append(table_md)\n",
    "            table_ranges_with_idx.append((table_start, table_end, len(all_table_markdown) - 1))\n",
    "            for block_idx in range(table_start, table_end + 1):\n",
    "                all_table_refs.add((block_idx, 0))\n",
    "        i += 1\n",
    "    # Build table list: (start_block, end_block, markdown)\n",
    "    table_list = [(start, end, all_table_markdown[idx]) for start, end, idx in table_ranges_with_idx]\n",
    "    logger.info(f\"log_146: Returning {len(table_list)} table ranges for page {page_num_1idx}\")\n",
    "    return table_list, all_table_refs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a294b91c",
   "metadata": {},
   "source": [
    "## extract_all_tables_from_blocks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "3b96e8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all_tables_from_blocks(all_blocks_with_raw_lines, page_num_1idx):\n",
    "    \"\"\"\n",
    "    Key Logic:\n",
    "    - Header blocks (extended or single-block ranges) are extracted to get column thresholds\n",
    "    - Data blocks following headers are extracted using the HEADER's thresholds\n",
    "    - Header and data rows are COMBINED into ONE table before formatting\n",
    "    - This ensures only ONE header and separator per logical table\n",
    "    - Handles extended headers (e.g., blocks 11-12 merged into single header range)\n",
    "    \n",
    "    Args:\n",
    "        all_blocks_with_raw_lines: List of (block_idx, raw_lines) tuples\n",
    "        page_num_1idx: Page number (1-indexed, for debugging)\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (markdown_table_str, set of (block_idx, line_idx) table references)\n",
    "    \"\"\"\n",
    "    logger.info(f\"log_131: Starting extract_all_tables_from_blocks for page {page_num_1idx}\")\n",
    "    # Detect table block ranges\n",
    "    table_ranges = detect_table_blocks_in_page(all_blocks_with_raw_lines)\n",
    "    if not table_ranges:\n",
    "        logger.info(\"log_132: No table ranges detected, returning empty string and set\")\n",
    "        return \"\", set()\n",
    "    # Build a map of header ranges to their thresholds\n",
    "    header_thresholds_map = {}  # Maps header_end_idx -> thresholds\n",
    "    for idx, (table_start, table_end) in enumerate(table_ranges):\n",
    "        # Check if there's a next range that immediately follows this range\n",
    "        if idx + 1 < len(table_ranges):\n",
    "            next_start, next_end = table_ranges[idx + 1]\n",
    "            if next_start == table_end + 1:\n",
    "                # This range is followed by data blocks\n",
    "                # Extract thresholds from this header range\n",
    "                rows = extract_table_rows_from_blocks(all_blocks_with_raw_lines, table_start, table_end)\n",
    "                if rows:\n",
    "                    block_dict = dict(all_blocks_with_raw_lines)\n",
    "                    x_positions = []\n",
    "                    for block_idx in range(table_start, table_end + 1):\n",
    "                        header_block_lines = block_dict.get(block_idx, [])\n",
    "                        for line in header_block_lines:\n",
    "                            for span in line.get('spans', []):\n",
    "                                x = span.get('origin', [0, 0])[0]\n",
    "                                x_positions.append(x)\n",
    "                    clustered_x = []\n",
    "                    for x_val in sorted(set(x_positions)):\n",
    "                        if not clustered_x or abs(x_val - clustered_x[-1]) > 30:\n",
    "                            clustered_x.append(x_val)\n",
    "                    header_thresholds_map[table_end] = sorted(clustered_x)\n",
    "    # Extract and convert each table range\n",
    "    all_table_markdown = []\n",
    "    all_table_refs = set()\n",
    "    i = 0\n",
    "    while i < len(table_ranges):\n",
    "        table_start, table_end = table_ranges[i]\n",
    "        # Check if this range is a header (immediately followed by data blocks)\n",
    "        if table_end in header_thresholds_map and i + 1 < len(table_ranges):\n",
    "            next_start, next_end = table_ranges[i + 1]\n",
    "            if next_start == table_end + 1:\n",
    "                logger.info(f\"log_133: Header range {table_start}-{table_end} followed by data range {next_start}-{next_end}\")\n",
    "                header_thresholds = header_thresholds_map.get(table_end, None)\n",
    "                header_rows = extract_table_rows_from_blocks(all_blocks_with_raw_lines, table_start, table_end, header_thresholds)\n",
    "                data_rows = extract_table_rows_from_blocks(all_blocks_with_raw_lines, next_start, next_end, header_thresholds)\n",
    "                combined_rows = header_rows + data_rows\n",
    "                table_md = format_markdown_table(combined_rows)\n",
    "                if table_md:\n",
    "                    logger.info(f\"log_134: Table markdown generated for blocks {table_start}-{next_end}\")\n",
    "                    all_table_markdown.append(table_md)\n",
    "                    for block_idx in range(table_start, next_end + 1):\n",
    "                        all_table_refs.add((block_idx, 0))\n",
    "                i += 2  # Skip both header and data ranges\n",
    "                continue\n",
    "        header_thresholds = None\n",
    "        rows = extract_table_rows_from_blocks(all_blocks_with_raw_lines, table_start, table_end, header_thresholds)\n",
    "        table_md = format_markdown_table(rows)\n",
    "        if table_md:\n",
    "            logger.info(f\"log_135: Table markdown generated for blocks {table_start}-{table_end}\")\n",
    "            all_table_markdown.append(table_md)\n",
    "            for block_idx in range(table_start, table_end + 1):\n",
    "                all_table_refs.add((block_idx, 0))\n",
    "        i += 1\n",
    "    # Combine all table markdowns with newlines between them\n",
    "    combined_markdown = \"\\n\\n\".join(all_table_markdown) if all_table_markdown else \"\"\n",
    "    logger.info(\"log_136: Returning combined markdown and table refs\")\n",
    "    return combined_markdown, all_table_refs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c714d5c",
   "metadata": {},
   "source": [
    "## Main Extraction & Chapter Processing\n",
    "\n",
    "**extract_and_split_by_chapter()**:\n",
    "- Main orchestration function that processes the entire PDF\n",
    "- For each chapter in CHAPTER_MAP:\n",
    "  1. Initialize page state tracker\n",
    "  2. Parse each page using parse_page_with_metadata()\n",
    "  3. Track code block state across pages for continuity\n",
    "  4. Close any open code blocks at chapter end\n",
    "  5. Post-process to convert remaining bullet characters\n",
    "  6. Generate output filename with prefix (00, 01-25 for chapters, A-E for appendices)\n",
    "  7. Write markdown to file\n",
    "\n",
    "**Outputs**: One markdown file per chapter in `teradataml_user_guide/` directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "a0a024eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Extraction & Chapter Processing\n",
    "\n",
    "def extract_and_split_by_chapter(pdf_path, chapter_map, output_dir=\"teradataml_user_guide\"):\n",
    "    \"\"\"\n",
    "    Extract chapters from PDF and split into markdown files.\n",
    "    Handles multi-page code block continuity within chapters.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path: Path to PDF file\n",
    "        chapter_map: List of (title, start_page, end_page) tuples\n",
    "        output_dir: Output directory for markdown files\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "        total_pages = doc.page_count\n",
    "        logger.info(f\"log_001: Opened PDF {pdf_path} with {total_pages} pages\")\n",
    "\n",
    "        for idx, (title, start_page_1idx, end_page_1idx) in enumerate(chapter_map):\n",
    "            logger.info(f\"log_002: Processing chapter {idx}: '{title}' pages {start_page_1idx}-{end_page_1idx}\")\n",
    "            start_page_0idx = start_page_1idx - 1\n",
    "            end_page_0idx = end_page_1idx - 1\n",
    "\n",
    "            logger.info(f\"log_004: Starting processing for chapter {idx}: '{title}'\")\n",
    "            chapter_text = [f\"# {title}\\n\\n\"] \n",
    "            page_state = {'in_code_block': False, 'pending_lines': []}\n",
    "            \n",
    "            # Process each page in chapter\n",
    "            for page_num_0idx in range(start_page_0idx, end_page_0idx + 1):\n",
    "                logger.info(f\"log_005: Processing page {page_num_0idx + 1:04d} for chapter {idx}: '{title}'\")\n",
    "                page = doc.load_page(page_num_0idx)\n",
    "                result = parse_page_with_metadata(page, page_num_0idx + 1, page_state)\n",
    "                logger.info(f\"log_006: Parsed page {page_num_0idx + 1:04d} for chapter {idx}: '{title}'\")\n",
    "                \n",
    "                markdown_text = result['markdown']\n",
    "                page_state = result['state']\n",
    "                \n",
    "                chapter_text.append(markdown_text)\n",
    "                chapter_text.append('\\n')\n",
    "            \n",
    "            # Close any open code block at end of chapter\n",
    "            if page_state['in_code_block']:\n",
    "                logger.info(f\"log_007: Closing open code block for chapter {idx}: '{title}'\")\n",
    "                chapter_text.append(\"```\\n\")\n",
    "            \n",
    "            # Post-process: convert any remaining old bullet characters\n",
    "            final_text = \"\".join(chapter_text).strip()\n",
    "            final_text = convert_bullets_in_text(final_text)\n",
    "            logger.info(f\"log_008: Converted bullets for chapter {idx}: '{title}'\")\n",
    "            \n",
    "            # Determine output filename with prefix\n",
    "            if idx == 0:\n",
    "                prefix = \"00\"\n",
    "                logger.info(f\"log_009: Set prefix '00' for chapter {idx}: '{title}'\")\n",
    "            elif idx < APPENDIX_START_INDEX:\n",
    "                prefix = f\"{idx:02d}\"\n",
    "                logger.info(f\"log_010: Set prefix '{prefix}' for chapter {idx}: '{title}'\")\n",
    "            \n",
    "            safe_title = sanitize_title(title)\n",
    "            logger.info(f\"log_012: Sanitized title to '{safe_title}' for chapter {idx}: '{title}'\")\n",
    "            output_filename = os.path.join(output_dir, f\"{prefix}_{safe_title}.md\")\n",
    "            \n",
    "            # Write to file\n",
    "            with open(output_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(final_text)\n",
    "            logger.info(f\"log_013: Wrote file '{output_filename}' for chapter {idx}: '{title}'\")\n",
    "            print(f\"✓ Generated {output_filename}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.info(f\"log_014: Error during extraction: {e}\")\n",
    "        print(f\"Error during extraction: {e}\")\n",
    "    finally:\n",
    "        if 'doc' in locals() and doc:\n",
    "            doc.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788d59ed",
   "metadata": {},
   "source": [
    "## Helper Functions: Title Sanitization & Noise Removal\n",
    "\n",
    "**sanitize_title()**:\n",
    "- Cleans chapter titles for safe filenames\n",
    "- Removes chapter/appendix prefixes\n",
    "- Removes special characters, keeps word chars + parentheses/brackets\n",
    "- Replaces spaces with underscores\n",
    "- Example: \"Chapter 1: DataFrames Setup\" → \"DataFrames_Setup\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "f0ccdd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_title(title):\n",
    "    \"\"\"\n",
    "    Clean chapter titles for use in filenames.\n",
    "    \n",
    "    Steps:\n",
    "    1. Remove leading chapter/appendix prefix (e.g., \"Chapter 1:\", \"Appendix A:\")\n",
    "    2. Remove special characters (keep only word chars, spaces, parentheses, brackets, hyphens)\n",
    "    3. Replace spaces/hyphens with underscores\n",
    "    \n",
    "    Args:\n",
    "        title: Raw chapter title from PDF\n",
    "    \n",
    "    Returns:\n",
    "        Sanitized title safe for filenames\n",
    "    \n",
    "    Example:\n",
    "        \"Chapter 1: DataFrames Setup and Basics\" → \"DataFrames_Setup_and_Basics\"\n",
    "    \"\"\"\n",
    "    logger.info(f\"log_015: Sanitizing title '{title}'\")\n",
    "    title = re.sub(r'^(Chapter\\s\\d{1,2}|Appendix\\s[A-E]):\\s*', '', title, flags=re.IGNORECASE)\n",
    "    title = re.sub(r'[^\\w\\s()\\[\\]-]', '', title)\n",
    "    title = re.sub(r'[\\s-]+', '_', title).strip('_')\n",
    "    return title"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c515e1eb",
   "metadata": {},
   "source": [
    "## Clean Page Noise\n",
    "\n",
    "**clean_page_noise()**:\n",
    "- Removes header, footer, and page number artifacts from extracted text\n",
    "- Cleans up formatting issues like non-breaking spaces\n",
    "- Returns clean, readable text for further processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "68e1631c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_page_noise(text, page_number):\n",
    "    \"\"\"\n",
    "    Strip header/footer noise and non-breaking spaces from extracted text.\n",
    "    \n",
    "    Removes:\n",
    "    1. File header (\"Teradata® Package for Python User Guide, Release 20.00\")\n",
    "    2. Page numbers in headers/footers\n",
    "    3. Chapter title fragments used in headers\n",
    "    4. Multiple consecutive newlines\n",
    "    5. Non-breaking spaces (U+00A0)\n",
    "    \n",
    "    Args:\n",
    "        text: Raw extracted text from PDF page\n",
    "        page_number: Current page number (1-indexed, used to find page number in headers)\n",
    "    \n",
    "    Returns:\n",
    "        Clean text with noise removed\n",
    "    \"\"\"\n",
    "    logger.info(f\"log_016: {page_number:04d} Cleaning page noise\")\n",
    "    FILE_NAME_REGEX = re.escape(\"Teradata® Package for Python User Guide, Release 20.00\")\n",
    "    text = re.sub(FILE_NAME_REGEX, '', text)\n",
    "    page_num_regex = re.escape(str(page_number))\n",
    "    text = re.sub(rf'\\s*{page_num_regex}\\s*', '\\n', text)\n",
    "    chapter_title_fragments = [\n",
    "        r'Context to Teradata Vantage', r'teradataml DataFrame Column', r'Executing Python Functions Inside Database',\n",
    "        r'DataFrames for Tables and Views', r'teradataml Window Aggregates', r'teradataml Options',\n",
    "        r'teradataml Utility and General Functions', r'Engine 20', r'Table and Views',\n",
    "        r'Installing, Uninstalling, and Upgrading Teradata Package for Python'\n",
    "    ]\n",
    "    fragment_pattern = r'|'.join(re.escape(f) for f in chapter_title_fragments)\n",
    "    noise_regex = re.compile(\n",
    "        r'^\\s*(\\d{1,2}:\\s*.*(?:' + fragment_pattern + r').*|.*(?:' + fragment_pattern + r')\\s*\\d{1,2}\\s*)\\s*$', \n",
    "        flags=re.IGNORECASE | re.MULTILINE\n",
    "    )\n",
    "    text = re.sub(noise_regex, '', text).strip()\n",
    "    text = re.sub(r'\\n\\s*\\n', '\\n\\n', text).strip()\n",
    "    text = text.replace('\\u00a0', ' ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3c0f95",
   "metadata": {},
   "source": [
    "## Determine Code Lines\n",
    "\n",
    "**is_code_line()**:\n",
    "- Distinguishes between Python code and execution output\n",
    "- Checks for REPL prompts and code font characteristics\n",
    "- Returns boolean indicating if a line contains code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "65c19066",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_code_line(line_text_clean, line_font):\n",
    "    \"\"\"\n",
    "    Determine if a line is actual Python code (vs output from code execution).\n",
    "    \n",
    "    A line is considered code if:\n",
    "    1. It's in the CODE_FONT (Consolas)\n",
    "    2. It starts with Python REPL prompt (>>> or ...)\n",
    "    \n",
    "    Used to distinguish between:\n",
    "    - Code: \">>> result = df.head()\"\n",
    "    - Output: \"Name  Age  Score\"\n",
    "    \n",
    "    Args:\n",
    "        line_text_clean: The text content of the line (already trimmed)\n",
    "        line_font: Font name extracted from PDF (e.g., 'Consolas', 'Arial')\n",
    "    \n",
    "    Returns:\n",
    "        True if line is code (has REPL prompt), False if it's output\n",
    "    \"\"\"\n",
    "    logger.info(f\"log_137: Checking if code line with font '{line_font}' and text '{line_text_clean}'\")\n",
    "    if line_font != CODE_FONT:\n",
    "        logger.info(\"log_138: Font does not match CODE_FONT, returning False\")\n",
    "        return False\n",
    "    result = line_text_clean.startswith(('>>>', '...'))\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131f3188",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "00ee9a77",
   "metadata": {},
   "source": [
    "## Export Raw page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "4bda0d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_raw_page_metadata(pdf_path, page_num_1idx, output_filename):\n",
    "    \"\"\"Export raw PDF metadata for a specific page (for debugging).\"\"\"\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "        if 0 < page_num_1idx <= doc.page_count:\n",
    "            page_num_0idx = page_num_1idx - 1\n",
    "            page = doc.load_page(page_num_0idx)\n",
    "            data = page.get_text(\"dict\")\n",
    "            with open(output_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(json.dumps(data, indent=4))\n",
    "            print(f\"✨ Successfully exported raw metadata for page {page_num_1idx} to '{output_filename}'.\")\n",
    "        else:\n",
    "            print(f\"Error: Page number {page_num_1idx} is out of bounds.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: PDF file not found at '{pdf_path}'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during metadata export: {e}\")\n",
    "    finally:\n",
    "        if 'doc' in locals() and doc:\n",
    "            doc.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "8d6b277a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✨ Successfully exported raw metadata for page 204 to 'page_204_raw_metadata_debug.txt'.\n"
     ]
    }
   ],
   "source": [
    "export_raw_page_metadata(PDF_FILE, 204, \"page_204_raw_metadata_debug.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f14fcb",
   "metadata": {},
   "source": [
    "## Main Extraction & Chapter Processing\n",
    "\n",
    "**extract_and_split_by_chapter()**:\n",
    "- Main orchestration function that processes the entire PDF\n",
    "- For each chapter in CHAPTER_MAP:\n",
    "  1. Initialize page state tracker\n",
    "  2. Parse each page using parse_page_with_metadata()\n",
    "  3. Track code block state across pages for continuity\n",
    "  4. Close any open code blocks at chapter end\n",
    "  5. Post-process to convert remaining bullet characters\n",
    "  6. Generate output filename with prefix (00, 01-25 for chapters, A-E for appendices)\n",
    "  7. Write markdown to file\n",
    "\n",
    "**Outputs**: One markdown file per chapter in `teradataml_user_guide/` directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0325a0b",
   "metadata": {},
   "source": [
    "## Export Raw Page Metadata\n",
    "\n",
    "This cell exports raw PDF metadata for page 204 for debugging purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa2a4ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Generated teradataml_user_guide/00_Table_of_Contents.md\n",
      "✓ Generated teradataml_user_guide/01_Introduction_to_Teradata_Package_for_Python.md\n",
      "✓ Generated teradataml_user_guide/02_Installing_Uninstalling_and_Upgrading_Teradata_Package_for_Python.md\n",
      "✓ Generated teradataml_user_guide/03_teradataml_Components.md\n",
      "✓ Generated teradataml_user_guide/04_DataFrames_Setup_and_Basics_(Sources_Non_Default_DB_UAF).md\n",
      "✓ Generated teradataml_user_guide/05_DataFrame_Manipulation_(Core_API).md\n",
      "✓ Generated teradataml_user_guide/06_DataFrame_Metadata_Rotation_Saving_and_Export.md\n",
      "✓ Generated teradataml_user_guide/07_Executing_Python_Functions_Inside_Database_Engine_20.md\n",
      "✓ Generated teradataml_user_guide/08_teradataml_DataFrame_Column.md\n",
      "✓ Generated teradataml_user_guide/09_teradataml_Window_Aggregates.md\n",
      "✓ Generated teradataml_user_guide/10_Context_to_Teradata_Vantage.md\n",
      "✓ Generated teradataml_user_guide/11_teradataml_Options.md\n",
      "✓ Generated teradataml_user_guide/12_teradataml_Utility_and_General_Functions.md\n",
      "✓ Generated teradataml_user_guide/13_teradataml_Open_Source_Machine_Learning_Functions.md\n",
      "✓ Generated teradataml_user_guide/14_Script_Methods_(SCRIPT_Table_Operator).md\n",
      "✓ Generated teradataml_user_guide/15_Series_(DataFrame_Column_Sequence).md\n"
     ]
    }
   ],
   "source": [
    "# Execution: Extract and Process PDF\n",
    "\n",
    "if os.path.exists(PDF_FILE):\n",
    "    extract_and_split_by_chapter(PDF_FILE, CHAPTER_MAP)\n",
    "    print(\"\\n✅ Extraction complete with all bullet characters converted to markdown!\")\n",
    "else:\n",
    "    print(f\"Error: PDF file not found at '{PDF_FILE}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd7fffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✨ Successfully exported raw metadata for page 286 to 'page_286_raw_metadata_debug.txt'.\n"
     ]
    }
   ],
   "source": [
    "# Export raw metadata for page 12 to debug table issue\n",
    "export_raw_page_metadata(PDF_FILE, 286, \"page_286_raw_metadata_debug.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d8d798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f235aed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f012fea2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b072970",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311gcopilot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
